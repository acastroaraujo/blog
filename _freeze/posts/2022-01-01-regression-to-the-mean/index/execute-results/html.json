{
  "hash": "343677644c9de9c09570f9948b37c95f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Regression to the Mean\"\ndescription: |\n  A visual explanation.\nauthor: andrés castro araújo\ncategories:\n  - Statistics\n  - Causality\ndate: 2022-01-01\nbibliography: references.bib\n---\n\n\n\n\n## Origin story\n\nRegression to the mean is one of the greatest parables in statistics. The term originated in 1886, when Francis Galton was studying hereditary patterns in human populations. While he was studying the association between children's and parent's heights, he noticed that tall (short) parents tend to have children shorter (taller) than themselves; and that they tended towards a *population average*.\n\n<aside>**par-a-ble**: A simple story used to illustrate a moral or spiritual lesson, as told by Jesus in the Gospels.</aside>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"show\"}\nHistData::Galton |> \n  ggplot(aes(x = parent, y = child)) + \n  geom_jitter(alpha = 0.5) +\n  ## line of equality\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  ## regression line\n  geom_smooth(method = lm, se = FALSE, color = \"skyblue\") \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThis is the data originally analyzed by Galton himself. The dashed line represents [*equality*]{style=\"color:red\"}, meaning that parents and children have the same height. But Galton noted that children's heights were actually closer to the blue [*prediction line*]{style=\"color:skyblue\"}.\n\n<aside>See @stigler2016.</aside>\n\nThe key insight is that, whenever randomness is involved, the more extreme outcomes will tend to be followed by more moderate outcomes. Why? Because part of the reason these outcomes are so extreme in the first place is due to randomness. Galton also termed this phenomena \"regression to mediocrity\"[^1].\n\n[^1]: Historically, there have been two kinds moral judgments regarding averages and normal distributions [@hacking1990]: (1) the Quetelet-Durkheim conception of the normal as the *right* and the *good*; and (2) Galton's notion of the normal as the *mediocre*, and *in need of improvement*.\n\nNowadays, we would look at this same problem using linear regression, which allows us to put the predicted heights of children into a simple formula.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- HistData::Galton |> \n  mutate(parent_centered = parent - mean(parent))\n\nOLS <- lm(child ~ parent_centered, data = d) ## scaling\n\ncoefficients(OLS)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    (Intercept) parent_centered \n     68.0884698       0.6462906 \n```\n\n\n:::\n:::\n\n\nNote that the heights of parents have been centered in order to produce an intercept that corresponds to the population average (or \"mediocrity\").\n\n$$\n\\widehat h_\\text{child} =  \\underbrace{68.09}_\\text{mediocrity} + 0.65 \\times \\widetilde h_\\text{parent} \n$$\n\nAt first encounter, some people will interpret this as meaning that *regression to the mean* implies that heights will become \"more average\" over time. Thus, a parent who is 5 inches above average is predicted to have children 3.2 inches taller than average, who in turn are predicted to have children 2.1 inches taller than average, and so on. This very common misunderstanding arises from neglecting the error term when thinking about future observations.\n\n$$\nh_\\text{child} = \\widehat h_\\text{child} + \\text{error}\n$$\n\nIn Gelman et al's words:\n\n> The point predictions regress toward the mean---that's the coefficient less than 1---and this reduces variation. At the same time, though, the error in the model---the imperfection of the prediction---*adds* variation, just enough to keep the total variation in height roughly constant from one generation to the next.\n>\n> Regression to the mean thus will always arise in some form whenever predictions are imperfect in a stable environment. The imperfection of the prediction induces variation, and regression in the point prediction is required in order to keep the total variation constant.\n>\n> @gelman2020 [pp. 88]\n\nThe following graphs show both ways of interpreting *regression to the mean*, based on how the data could look like across 12 generations:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nOLS <- lm(child ~ parent, data = HistData::Galton)\nintercept <- coefficients(OLS)[[1]]\nslope <- coefficients(OLS)[[2]]\nsigma <- sd(residuals(OLS))\n\ng0 <- HistData::Galton$parent # first ancestors\n\n## Simulation\n\nreg_naive <- function(x, ...) {\n  x * slope + intercept\n}\n\nreg_correct <- function(x, ...) {\n  rnorm(n = length(x), mean = x * slope + intercept, sd = sigma)\n}\n\nn <- 12 # number of generations\n\ns_naive <- accumulate(1:n, reg_naive, .init = g0) \ns_correct <- accumulate(1:n, reg_correct, .init = g0)\nnames(s_correct) <- names(s_naive) <- 0:n\n\ndf_naive <- as_tibble(s_naive) |> \n  mutate(ancestor = row_number()) |> \n  pivot_longer(cols = !ancestor, names_to = \"generation\", values_to = \"height\") |> \n  mutate(simulation = \"naive interpretation\", generation = as.integer(generation))\n\ndf_correct <- as_tibble(s_correct) |> \n  mutate(ancestor = row_number()) |> \n  pivot_longer(cols = !ancestor, names_to = \"generation\", values_to = \"height\") |> \n  mutate(simulation = \"correct interpretation\", generation = as.integer(generation))\n\nbind_rows(df_correct, df_naive) |> \n  ggplot(aes(x = generation, y = height, color = ancestor, group = ancestor)) +\n  geom_line(show.legend = FALSE, alpha = 1/10) +\n  facet_wrap(~ simulation, ncol = 1, scales = \"free_y\") + \n  labs(y = \"height (inches)\") + \n  scale_color_viridis_c() +\n  scale_x_continuous(breaks = 0:12) +\n  theme(strip.text.x = element_text(size = 14)) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n<aside>\n\nThis very simple simulation assumes that the population average remains constant over time, which is obviously false when you consider changes in public health, such as nutrition.\n\nAs a general rule, *nothing in the social and biological sciences remains constant over very long periods of time*.\n\n</aside>\n\n## The lesson\n\n***Regression to the mean will trick people into seeing causality where there is none.***\n\nAs mentioned earlier, this phenomena shows up all the time wherever randomness (or \"luck\") is expected to play a role. Thus, we see it in everything from sports to public policy.[^2] This is a problem because human beings are inclined to find amazing patterns when they look at randomness. Obviously, this is mostly harmless when, for example, the source of randomness is clouds in the sky and the patterns we find take the form of \"a cow\" or \"a face.\"[^3] But in other settings, this same phenomena can become worrisome.\n\n[^2]: This also seems to be the reason why so many bands appear to suffer from \"second album syndrome\" or \"[sophomore slump](https://en.wikipedia.org/wiki/Sophomore_slump)\".\n\n[^3]: See: [**pareidolia**](https://en.wikipedia.org/wiki/Pareidolia).\n\n**An exercise in simulation**[^4]\n\n[^4]: This my solution to exercise 6.8 in *Regression and Other Stories*.\n\nA very famous real-world example is contained in an article titled \"On the psychology of prediction\" Tversky & Kahneman, 1973):\n\n> The instructors in a flight school adopted a policy of consistent positive reinforcement recommended by psychologists. They verbally reinforced each successful execution of a flight maneuver. After some experience with this training approach, the instructors claimed that contrary to psychological doctrine, **high praise for good execution of complex maneuvers typically results in a decrement of performance on the next try**. What should the psychologist say in response?\n\nTo answer this question we simulate data from 500 pilots, each of whom performs two maneuvers, and with each maneuver scored continuously on a 0-10 scale. Each pilot has a \"true ability\" that is unchanged during the two tasks, and the score for each test is equal to this true ability plus an independent error. Pilots get praised when they score higher than 7 during the first maneuver, and receive negative reinforcement when they score lower than 3.\n\nThe connection between this example and Galton's study is not obvious at first glance. Here, each pilot's score exhibits regression to her \"true ability\", in the sense that we expect \"luck\" to play an important role. As before, we don't really expect these \"true abilities\" to remain constant over time. In fact that would defeat the entire purpose of flight school. But we do expect these underlying abilities to change *very slowly*.\n\nNote that we make sure, by design, that **reinforcement has no effect on performance on the second task.**\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nN <- 500\ntrue_abilities <- rnorm(N, 0, 1)\n# mapping the true abilities + random noise to a [0,10] scale\ntask1 <- plogis(true_abilities + rnorm(N, 0, 0.5)) * 10\ntask2 <- plogis(true_abilities + rnorm(N, 0, 0.5)) * 10\n\ndf <- tibble(true_abilities, task1, task2) |> \n  mutate(reinforcement = case_when(\n    task1 < 3 ~ \"negative\",\n    task1 > 7 ~ \"positive\",\n    TRUE ~ \"neutral\")\n  )\n\ndf |> \n  ggplot(aes(x = task1, y = task2, color = reinforcement)) +\n  geom_point() +\n  scale_color_manual(values = c(\"pink\", \"grey\", \"skyblue\")) +\n  scale_x_continuous(breaks = seq(2, 10, 2)) +\n  scale_y_continuous(breaks = seq(2, 10, 2))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nWe can now compute the average change in scores for each group of pilots, which the instructors interpret to be the causal effect of reinforcement.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndf |> \n  mutate(change = task2 - task1) |> \n  group_by(reinforcement) |> \n  summarise(\n    effect = mean(change),\n    se = sd(change) / sqrt(n())\n    ) |> \n  ggplot(aes(x = reinforcement, y = effect)) +\n  geom_pointrange(aes(\n    ymin = effect + qnorm(p = 0.025)*se,\n    ymax = effect + qnorm(p = 0.975)*se\n    )) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nNotice that, on average, the pilots who were praised did worse on the second task, whereas the pilots who received negative reinforcement did better. But we know that such effect doesn't exist. And we know this because we *created* these data so that *task 1* and *task 2* were unrelated. The \"causal pattern\" we have just observed is a consequence of the noise in the data. Pilots who scored very well on *task 1* are likely to have a higher skill *and also* to have been somewhat lucky. Thus, it makes sense that they perform slightly worse on *task 2*.\n\nThis is how Tversky and Kahneman (1982) explain it:\n\n> Regression is inevitable in flight maneuvers because performance is not perfectly reliable and progress between successive maneuvers is slow. Hence, pilots who did exceptionally well on one trial are likely to deteriorate on the next, regardless of the instructors' reaction to the initial success. The experienced flight instructors actually discovered the regression but attributed it to the detrimental effect of positive reinforcement. ***This true story illustrates a saddening aspect of the human condition. We normally reinforce others when their behavior is good and punish them when their behavior is bad. By regression alone, therefore, they are most likely to improve after being punished and most likely to deteriorate after being rewarded.*** Consequently, we are exposed to a lifetime schedule in which we are most often rewarded for punishing others, and punished for rewarding.\n>\n> Quoted in @gelman2020 [pp. 90]\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}