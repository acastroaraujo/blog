{
  "hash": "ea2ca261cc70a69c1505d5a3327b5dd3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"cutting board\"\nexecute: \n  eval: false\n---\n\n\nReplace \\* with $\\top$ !\n\nA *singular value decomposition* (SVD) is a factorization of the form:\n\n$$\n\\mathbf{A} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^*\n$$\n\n<aside>definition</aside>\n\nwhere\n\n-   $\\mathbf{A}$ is an $m\\times n$ matrix with rank $r$\n\n-   $\\mathbf{U}$ is an $m\\times r$ with $\\mathbf{U}^*\\mathbf{U}=\\mathbf{I}_r$\n\n-   $\\mathbf{V}$ is an $n\\times r$ matrix with $\\mathbf{V}^*\\mathbf{V}=\\mathbf{I}_r$\n\n-   $\\boldsymbol \\Sigma$ is a diagonal matrix comprised of *singular values* ($\\sigma_1, \\sigma_2, \\dots, \\sigma_r$)\n\n-   The singular values are sorted such that $\\sigma_1 \\geq \\sigma_2 \\geq \\dots >0$\n\nThe ***rank*** ***k*** ***approximation*** of $\\mathbf{A}$ is given by the following sum:\n\n$$\n\\mathbf{A} \\approx \\sigma_1 \\cdot \\mathbf{u}_1 \\mathbf{v}_1^* + \\dots + \\sigma_k \\cdot \\mathbf{u}_k \\mathbf{v}_k^*\n$$\n\nThe columns of $\\mathbf U$ and $\\mathbf V$ are usually called \"left\" and \"right\" singular vectors.\n\nSVD is a bit magical.\n\n-   $\\underset{n \\times n}{\\mathbf A^\\top \\mathbf A} = \\mathbf V \\boldsymbol \\Sigma^2 \\mathbf V^\\top$\n\n-   $\\underset{m \\times m}{\\mathbf{AA}^\\top} = \\mathbf{U \\Sigma}^2 \\mathbf U^\\top$\n\nThis is related to eigen-vector decomposition!\n\nEver heard of principal components analysis (PCA)? It's literally SVD if we first center the columns of $\\mathbf A$ such that each one as a mean of zero.\n\nIn fact, if you check the documentation for `prcomp()` you'll see the following description:\n\n\"The calculation is done by a singular value decomposition of the (centered and possibly scaled) data matrix, not by using eigen() on the covariance matrix. This is generally the preferred method for numerical accuracy.\"\n\n**Example**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(davis, package = \"latentnet\")\nM <- network::as.matrix.network(davis)\n\nM \n\nsvddecomp <- svd(M)\nk <- 3\n\nwith(svddecomp, u[, 1:k] %*% diag(d[1:k]) %*% t(v[, 1:k]))\n```\n:::\n\n\n<file:///Users/acastroaraujo/Repositories/notebooks/_site/posts/2021-04-01-linear-algebra-basics/index.html>\n\n<https://rpubs.com/aaronsc32/singular-value-decomposition-r>\n\n<https://en.wikipedia.org/wiki/Singular_value_decomposition>\n\nSimilarity with word embedding\n\n-   The columns of **V** (referred to as right-singular vectors) are [eigenvectors](https://en.wikipedia.org/wiki/Eigenvectors \"Eigenvectors\") of **M**^⁎^**M**.\n\n-   The columns of **U** (referred to as left-singular vectors) are eigenvectors of **MM**^⁎^.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(igraph)\n\nout <- graph_from_biadjacency_matrix(M) |> \n  igraph::bipartite_projection(multiplicity = TRUE)\n\neigen_centrality(out$proj1, scale = FALSE)$vector |> sort()\n\n(svddecomp$v[, 1]) |> sort()\n\nproj <- M %*% t(M)\ndiag(proj) <- 0\n\nedecomp <- eigen(proj) \n\n(edecomp$vectors[, 1] * max(edecomp$values)) |> sort()\n\nsvddecomp$v[, 1] * svddecomp$d[[1]]\n\n## first comparison\nedecomp$vectors[, 1] |> abs() |> sort()\neigen_centrality(out$proj1, scale = FALSE)$vector |> sort()\n\n## second\nedecomp <- eigen(M %*% t(M)) \nsvddecomp <- svd(M)\nabs(edecomp$vectors[, 1]) |> sort()\nabs(svddecomp$u[, 1]) |> sort()\n\n## third\nout <- igraph::graph_from_biadjacency_matrix(M)\n\nout |> \n  igraph::authority_score(scale = FALSE) |> \n  purrr::pluck(\"vector\") |> \n  sort()\n\n## extra\nwith(edecomp, vectors %*% t(vectors)) |> round(digits = 10)\n```\n:::\n\n\nFrench stuff\n\n@breiger1974, @breiger2000\n\nand the correlation stuff\n\nand maybe this:\n\n<https://pages.stat.wisc.edu/~karlrohe/netsci/whyDoesSpectralWork.html>\n\n**Coleman and Bourdieu: Dual equilibria in a social field**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMAT <- rbind(\n    c(.14, .11, .15, .17, .13),\n    c(.14, .13, .13, .16, .13),\n    c(.13, .13, .13, .12, .13),\n    c(.13, .11, .11, .14, .13),\n    c(.09, .12, .11, .14, .11),\n    c(.11, .12, .10, .10, .05),\n    c(.10, .10, .11, .07, .16),\n    c(.11, .10, .10, .07, .08),\n    c(.06, .08, .06, .03, .08)\n)\n\ncolnames(MAT) <- c(\"crime\", \"economy\", \"civil_rights\", \"first_amend\", \"due_process\")\n\nrownames(MAT) <- c(\"rehnquist\", \"kennedy\", \"oconnor\", \"scalia\", \"stevens\", \"blackmun\", \"white\", \"souter\", \"thomas\")\n\ntotal <- c(256, 287, 192, 58, 38)\n\nM <- round(sweep(MAT, 2, total, FUN = \"*\"))\n\nsvd(M)\nsvd(MAT)\n\nX <- prop.table(M, margin = 1) |> t()\nC <- prop.table(M, margin = 2) \n\neigen(C %*% X)\neigen(X %*% C)\n\neigen(M %*% t(M))$vectors\nwith(out, u %*% diag(d[1:2]))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(davis, package = \"latentnet\")\nM <- network::as.matrix.network(davis)\n\nsvddecomp <- svd(M)\n\nwith(svddecomp, u %*% diag(d) %*% t(v))\n```\n:::\n\n\n\"Coleman's mathematical solutions for the 'power' of actors and for the 'value' of events are eigenvectors of products of his C and X matrices, even though he never quite says so.\" ???\n\n\n::: {.cell}\n\n```{.r .cell-code}\nigraph::graph_from_biadjacency_matrix(M) |> \n    igraph::hub_score(scale = FALSE) |> \n    purrr::pluck(\"vector\") |> \n    sort()\n```\n:::\n\n\nGoodman, Leo A., 1996. A single general method for the analysis of cross-classified data: Reconciliation and synthesis of some methods of Pearson, Yule, and Fisher and also some methods of correspondence analysis and association analysis. Journal of the American Statistical Association 91, 408--428.\n\nGoodman, Leo A. 1997. Statistical methods, graphical displays, and Tukey's ladder of re-expression in the analysis of nonindependence in contingency tables: Correspondence analysis, association analysis, and the midway view of nonindependence. In: D.R. Brillinger, L.T. Fernholz and S. Morgenthaler eds., The practice of data analysis: Essays in honor of John W. Tukey, 101-132. Princeton, NJ: Princeton University Press.\n\nThe french stuff by Holmes.\n\nHigher order.\n\nhttps://en.wikipedia.org/wiki/Higher-order_singular_value_decomposition\n\nL. R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31:279–311, 1966\n\nL. de Lathauwer, B. de Moor, and J. Vandewalle. A multilinear singular value decomposition. SIAM Journal of Matrix Analysis and Applications, 21(4):1253–78, 2000.\n\n\\[23\\] L. Grasedyck. Hierarchical singular value decomposition of tensors. SIAM Journal on Matrix Analysis and Applications, 31(4):2019–54, 2010.\n\n\\[24\\] W. Hackbusch and S. Kühn. A new scheme for the tensor representation. Journal of Fourier Analysis and Applications, 15(5):706–722, 2009.\n\nI. V. Oseledets. Tensor-train decomposition. SIAM Journal on Scientific Computing, 33(5):2295–2317, 2011.\n\nMultiple Invariants and Generalized Rank of a P-Way Matrix or Tensor\n\nhttps://en.wikipedia.org/wiki/Multilinear_principal_component_analysis\n\nMartin & Porter: The Extraordinary SVD\nhttps://people.maths.ox.ac.uk/~porterm/papers/s4.pdf\n\n***\n\nLSA r package",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}