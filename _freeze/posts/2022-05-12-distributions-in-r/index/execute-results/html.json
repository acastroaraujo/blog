{
  "hash": "73aae122717426d6f1ead91a8e85ea95",
  "result": {
    "markdown": "---\ntitle: \"Distributions in R\"\ndescription: |\n  A short tutorial.\nauthor: andrés castro araújo\ncategories: \n  - R\n  - Probability\ndate: 2022-05-12\nbibliography: references.bib\n---\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Set up\"}\nlibrary(tidyverse)\ntheme_set(\n  theme_light(base_family = \"Optima\") + \n  theme(strip.background = element_rect(fill = \"#595959\"))\n)\n```\n:::\n\n\nR has four built-in forms of working with probability distributions.\n\nSee `?distributions` for a list of common distributions contained in R.\n\nFor example, to work with a normal distribution we have the following functions:\n\n-   `dnorm()`: probability (density) function\n\n-   `pnorm()`: cumulative distribution function\n\n-   `rnorm()`: draw `n` samples from `dnorm()`\n\n-   `qnorm()`: quantile distribution; this is the *inverse* of `pnorm()`\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    pnorm(q = 1.25)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    [1] 0.8943502\n    ```\n    :::\n    \n    ```{.r .cell-code}\n    qnorm(p = 0.8943502)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    [1] 1.25\n    ```\n    :::\n    :::\n\n\nA common source of confusion comes from the difference between *continuous* random variables (e.g. a normal distribution) and *discrete* random variables (e.g. a binomial distribution).\n\n-   For a **discrete variable** it's easier to think about probability distributions first (or *probability mass functions*); then the cumulative distribution is just the cumulative sum of different probability masses.\n\n-   For **continuous variables** it's easier to think about cumulative distributions first; then the probability distribution (or *probability density function*) is just the *derivative*---or \"slope\"---of the cumulative distribution.\n\nThis will all make sense.\n\n## discrete distributions\n\nIn this section we'll use two distributions as examples.\n\nThe probability distribution of a binomial random variable comes from adding coin flips (also known as *Bernoulli* distributions). The Bernoulli distribution has two possible outcomes $x = \\{0, 1\\}$ and one parameter $p$ (which confusingly is *also* a probability).\n\nFor example, let's suppose a coin is loaded and so $p = 0.75$.\n\nThe probability mass function (PMF) of **this** *Bernoulli distribution* is as follows:\n\n$$\nf(x) = \\Pr(X = x) = \\begin{cases} \n    0.75 &\\text{if} \\ x = 1 \\\\\\\\\n    0.25 &\\text{if} \\ x = 0 \\\\\\\\\n    0 &\\text{if} \\ x = \\text{anything else}\n\\end{cases}\n$$\n\nThe cumulative distribution function (CDF) is as follows:\n\n$$\nF(x) = \\Pr(X \\leq x) = \\begin{cases} \n    0 &\\text{if} \\ x < 1 \\\\\n    0.25 &\\text{if} \\ x = 0 \\\\\n    1  &\\text{if} \\ x = 1 \\\\\n    1 &\\text{if} \\ x > 1 \n\\end{cases}\n$$\n\n<aside>Note the change in notation from $f$ to $F$.</aside>\n\nAny CDF returns the probability that an outcome is less than or equal to $x$. In other words, you're simply adding up the probability masses for each possible outcome until you reach $x$.\n\n**The binomial distribution**\n\nAs mentioned earlier, the binomial distribution comes from adding $n$ coin flips. For example, if you throw 3 coins then we have four possible outcomes $x = \\{0, 1, 2, 3\\}$ and two parameters: $p$ and $n = 3$.\n\nThe probability (mass) function of **this** binomial distribution is then this:\n\n$$\nf(x) = \\Pr(X = x) = \\begin{cases} \n    1 \\ (1 - p)^3 &\\text{if} \\ x = 0 \\\\\n    3 \\ p(1 - p)^2 &\\text{if} \\ x = 1 \\\\ \n    3 \\ p^2(1-p) &\\text{if} \\ x = 2 \\\\\n    1\\ p^3 &\\text{if} \\ x = 3 \\\\\n    0 &\\text{if} \\ x = \\text{anything else}\n\\end{cases}\n$$\n\nThe 1s and 3s come from counting the number of ways in which $x$ can equal one of these numbers. This is not different from \"the garden of forking data\" stuff in @mcelreath2020 [pp. 20-7]\n\nBut this is not how you'll see binomial distributions written out in the wild. We need new notation in order to write **any** binomial distribution, which we get by using the binomial coefficient:\n\n$$\n{n \\choose x} = \\frac{n!}{x! (n - x)!}\n$$\n\nSo the probability (mass) function of **any** *binomial distribution* is then this:\n\n$$\nf(x) = \\Pr(X = x) = {n \\choose x} p^x (1-p)^{n-x}\n$$\n\nThe cumulative distribution function is as follows:\n\n$$\nF(x) = \\Pr(X \\leq x) = \\sum_{i = 0}^x {n \\choose x} p^x (1-p)^{n-x}\n$$\n\nFor example, with $n = 10$ and $p = 0.5$, this is how they look:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntibble(x = 0:10) |> \n  mutate(\n    \"Probability Mass Function — dbinom(x, size = 10, p = 1/2)\" = \n      dbinom(x, size = 10, p = 1/2),\n    \"Cumulative Distribution Function — pbinom(x, size = 10, p = 1/2)\" = \n      pbinom(x, size = 10, p = 1/2)\n  ) |>\n  mutate(x = factor(x)) |> \n  pivot_longer(!x, names_to = \"distribution\") |> \n  ggplot(aes(x, value)) + \n  geom_col(width = 1/3) +\n  facet_wrap(~distribution, ncol = 1) + \n  labs(y = NULL)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n*Note that the Bernoulli distribution is now a special case of the binomial distribution in which* $n = 1$*.*\n\nThis is what's going on when you use the `dbinom` and `pbinom` functions:\n\n*The Bernoulli PMF is the same as the binomial PMF with* $n = 1$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbinom(x = c(-2, -1, 0, 1, 2, 3), size = 1, prob = 0.75)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.00 0.00 0.25 0.75 0.00 0.00\n```\n:::\n:::\n\n\n*Bernoulli CDF*\n\n\n::: {.cell}\n\n```{.r .cell-code}\npbinom(q = c(-2, -1, 0, 1, 2, 3), size = 1, prob = 0.75)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.00 0.00 0.25 1.00 1.00 1.00\n```\n:::\n:::\n\n\n*Binomial PMF with* $n=4$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbinom(x = seq(-1, 5), size = 4, prob = 0.75)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.00000000 0.00390625 0.04687500 0.21093750 0.42187500 0.31640625 0.00000000\n```\n:::\n:::\n\n\n*Binomial CDF with* $n=4$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npbinom(q = seq(-1, 5), size = 4, prob = 0.75)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.00000000 0.00390625 0.05078125 0.26171875 0.68359375 1.00000000 1.00000000\n```\n:::\n:::\n\n\n*Note that because `pbinom` is just adding different pieces of `dbinom` together, we could have obtained the same results simply by adding.*\n\n*Binomial CDF with* $n = 4$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncumsum(dbinom(x = seq(-1, 5), size = 4, prob = 0.75))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.00000000 0.00390625 0.05078125 0.26171875 0.68359375 1.00000000 1.00000000\n```\n:::\n:::\n\n\n**Drawing random samples**\n\n`rbinom` is used to draw samples from `dbinom`. This makes doing math *very* easy. For example, suppose we have 12 coin flips---or a binomial distribution with $n = 12$ and $p = 0.5$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws <- rbinom(n = 1e4, size = 12, prob = 0.5) \n```\n:::\n\n\n*What is the probability that* $x = 7$*?*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(draws == 7)  ## approx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1914\n```\n:::\n\n```{.r .cell-code}\ndbinom(x = 7, size = 12, prob = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1933594\n```\n:::\n:::\n\n\n*What is the probability that* $x \\leq 8$*?*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(draws <= 8)  ## approx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9296\n```\n:::\n\n```{.r .cell-code}\npbinom(q = 8, size = 12, prob = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.927002\n```\n:::\n:::\n\n\n*What is the probability that* $x$ *is* $1$ *or* $4$ *or* $9$*?*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(draws %in% c(1, 4, 9)) ## approx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.175\n```\n:::\n\n```{.r .cell-code}\nsum(dbinom(x = c(1, 4, 9), size = 12, prob = 0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1774902\n```\n:::\n:::\n\n\n## continuous distributions\n\nThe well-known probability (density) distribution for a normal random variable has two parameters $\\mu$ and $\\sigma^2$.\n\nIt's ugly:\n\n$$\nf(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\bigg(- \\frac{(x - \\mu)^2}{2 \\sigma^2}\\bigg)\n$$\n\n*Note that* $f(x) \\neq \\Pr(X = x)$*.*\n\nBecause $x$ is a *real* number (that ranges from $-\\infty$ to $+\\infty$), the probability that $x = 1$ is exactly the same as the probability that $x = 0.9999...$ both are *zero*.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot() + \n  xlim(-5, 5) + \n  geom_function(fun = dnorm) + \n  labs(y = \"density\", x = \"x\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nHowever, the cumulative distribution function (CDF) *does* have the same interpretation. If you add all the possible values until you reach $x$ you get $\\Pr(X \\leq x)$. BUT, because there exists an infinite amount of numbers between $-\\infty$ and $x$, you can't simply add. You have to **integrate**.\n\n$$\nF(x) = \\Pr(X \\leq x) = \\int_{-\\infty}^x f(x) dx\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot() + \n  xlim(-5, 5) + \n  geom_function(fun = pnorm) + \n  labs(y = \"cumulative probability\", x = \"x\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nAssuming $\\mu = 0$ and $\\sigma = 2$, what is the probability that $x$ is less than or equal to zero?\n\nThe following two chunks of code give the same answer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(q = 0, mean = 0, sd = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5\n```\n:::\n\n```{.r .cell-code}\nintegrate(dnorm, lower = -Inf, upper = 0, mean = 0, sd = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.5 with absolute error < 7.3e-07\n```\n:::\n:::\n\n\nYou can also get an approximate answer by drawing random samples with `rnorm`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws <- rnorm(1e5, mean = 0, sd = 2)\nmean(draws <= 0) ## approx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.49868\n```\n:::\n:::\n\n\nKnowing that the CDF is an *integral,* we can understand the PDF as the *derivative* of the CDF. (The derivative of an integral of a function is just the function itself). In other words, a probability **density** is the rate of change in cumulative probability at $x$. The PDF is the \"slope\" of the CDF at $x$. This means that if the cumulative probability is increasing rapidly, the density can easily exceed 1. But if we calculate the area under the density function, it will never exceed 1.\n\n<aside>See the \"overthinking\" section in @mcelreath2020 [pp. 76] for a similar description of this issue.</aside>\n\nFor example, compare the PDF and CDF of the exponential distribution. While the CDF eventually converges to 1, the density easily exceeds 1 at some points.\n\n*CDF:*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + \n  xlim(0, 5) + \n  geom_function(fun = function(x) pexp(x, rate = 3)) + \n  labs(y = \"cumulative probability\", x = \"x\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n*PDF:*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + \n  xlim(0, 5) + \n  geom_function(fun = function(x) dexp(x, rate = 3)) +\n  labs(y = \"density\", x = \"x\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nMore examples:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws <- rnorm(1e4, mean = 2, sd = 5)\n```\n:::\n\n\n*What is the probability that* $x = 7$*?*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(draws == 7)  ## approx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\n*What is the probability that* $3 < x < 7$*?*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nintegrate(dnorm, mean = 2, sd = 5, lower = 3, upper = 7)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.262085 with absolute error < 2.9e-15\n```\n:::\n\n```{.r .cell-code}\nmean(3 < draws & draws < 7)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2566\n```\n:::\n:::\n\n\nWhat is the probability that $x \\leq 8$?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(draws <= 8)  ## approx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8888\n```\n:::\n\n```{.r .cell-code}\npnorm(8, mean = 2, sd = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8849303\n```\n:::\n:::\n\n\nWhat is the probability that $x$ is NOT between $-2$ and $2$?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - integrate(dnorm, mean = 2, sd = 5, lower = -2, upper = 2)$value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7118554\n```\n:::\n\n```{.r .cell-code}\nmean(draws < -2 | draws > 2)  ## approx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.708\n```\n:::\n:::\n\n\n## quantile functions\n\nThe inverse of a CDF is called a **quantile function (**$Q = F^{-1}$).\n\nThis is where we get stuff like the median:\n\n$$\n\\underbrace{Q(0.5)}_\\text{median} = x \\iff \\Pr(X \\leq x) = 0.5\n$$\n\nMedian example with the exponential distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqexp(p = 0.5, rate = 3)         ## finding the median\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2310491\n```\n:::\n\n```{.r .cell-code}\npexp(q = 0.2310491, rate = 3)   ## verifying the median\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5000001\n```\n:::\n\n```{.r .cell-code}\ndraws <- rexp(1e5, rate = 3)    ## finding the median using random draws\nquantile(draws, 0.5)            ## approx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      50% \n0.2295202 \n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}