---
title: "Optimization in R"
description: |
  A few different ways to do optimization.
author: andrés castro araújo
categories:
  - R
date: 01-24-2021
fig-height: 3
fig-width: 5
execute: 
  echo: true
---

*Finding the peak of parabola*

$$y = 15 + 10x - 2x^2$$

First we write this statement as an R function.

```{r}
parabola <- function(x) 15 + 10*x - 2*x^2 
```

Then we can visualize it using `curve()` or `ggplot2`.

```{r}
#| fig-align: center

library(ggplot2)
theme_set(theme_light(base_family = "Optima"))

g <- ggplot() + 
  geom_function(fun = parabola) +
  xlim(0, 5) +
  labs(x = "x", y = "f(x)")

g
```

Then we call `optimize()`, which takes the function as its first argument, the interval as its second, and an optional argument indicating whether or not you are searching for the function's maximum (minimize is the default).

```{r}
#| fig-align: center

out <- optimize(parabola, interval = c(-100, 100), maximum = TRUE)
out

g + geom_vline(xintercept = out$maximum, linetype = "dashed") +
    geom_hline(yintercept = out$objective, linetype = "dashed")
```

There are many more ways of using optimization in R. For example, if you want to find the maximum of a function with many parameters you can use `optim()`.

```{r}
opt <- optim(
  par = 99, ## initial values; use c(...) to do it with many parameters
  fn = parabola, 
  method = "BFGS",
  # this next line is critical: 
  # it tells R to maximize rather than minimize
  control = list(fnscale = -1)
)

opt
```

## Logistic Regression

In statistics we usually try to find the maximum of likelihood functions in order to fit regression models.

For example[^1], a simple logistic regression can be fit by doing the following:

[^1]: The data: «*A survey of 3020 residents in a small area of Bangladesh suffering from arsenic contamination of groundwater. Respondents with elevated arsenic levels in their wells had been encouraged to switch their water source to a safe public or private well in the nearby area and the survey was conducted several years later to learn which of the affected residents had switched wells»*

```{r}
## The dataset
data("wells", package = "rstanarm")
str(wells)

## The model
f <- formula(switch ~ dist + arsenic + dist:arsenic)

## The design matrix
X <- model.matrix(f, data = wells)
dim(X)

## The outcome variable
y <- wells$switch

## The log-likelihood function
log_likelihood <- function(beta, outcome, dmat) {
  
  linpred <- dmat %*% beta ## the linear predictor
  p <- plogis(linpred)     ## the link function
  
  sum(dbinom(outcome, size = 1, prob = p, log = TRUE))  ## the log-likelihood
}

## The maximum likelihood estimate (MLE)
opt <- optim(
  par = rep(0, ncol(X)), ## initial values are all 0's
  fn = log_likelihood, method = "BFGS",
  outcome = y, dmat = X,
  # this next line is critical: 
  # it tells R to maximize rather than minimize
  control = list(fnscale = -1)
)

names(opt$par) <- colnames(X)
opt$par
```

We can compare this to the outcome given by R's `glm()` function:

```{r}
fit <- glm(f, data = wells, family = binomial(link = "logit"))
coefficients(fit)
```

## Stan

Users of [**Stan**](https://mc-stan.org/) should know that it can be used for optimization as well.

```{r}
#| message: false
#| warning: false

library(cmdstanr)
mcmc_optim <- cmdstan_model("simple_parabola.stan")
mcmc_optim$print()

fit <- mcmc_optim$optimize()
fit
```
