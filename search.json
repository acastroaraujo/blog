[
  {
    "objectID": "posts/2023-12-27-what-is-computational-social-science/index.html",
    "href": "posts/2023-12-27-what-is-computational-social-science/index.html",
    "title": "What is Computational Social Science?",
    "section": "",
    "text": "I don’t think Computational Social Science (CSS) will ever become an autonomous field. Right now, CSS is more akin to a “trading zone” in which different disciplinary cultures manage to exchange ideas, metaphors, and techniques.\nAs such, there’s no point in creating exclusionary boundaries around CSS. That would be bad for trade.1\nMore importantly, CSS is distinct from previous interdisciplinary efforts in that it is mainly driven by advances in digital technology and it is associated with a new professional group."
  },
  {
    "objectID": "posts/2023-12-27-what-is-computational-social-science/index.html#footnotes",
    "href": "posts/2023-12-27-what-is-computational-social-science/index.html#footnotes",
    "title": "What is Computational Social Science?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMatt Salganik has made this point before, although for slightly different reasons; he jokingly argues that we should be happy with defining CSS simply as “anything that’s cool.” See An Introduction to Computational Social Science.↩︎\nThis idea goes back to the historian Peter Galison.↩︎\nSpector et al. (2022, p. 7) as “the study of extracting value from data—value in the form of insights or conclusions.” It’s impossible to create exclusionary boundaries around such a definition. But that should not matter as long as companies continue to recruit and students continue to enroll.↩︎\nSocial scientists have been thinking about text-as-data for ages (e.g., Markoff et al. 1975).↩︎\nSee Kline (2015) for cybernetics. See Thagard (2023) for cognitive science.↩︎"
  },
  {
    "objectID": "posts/2022-02-07-theory-work/theory-work.html",
    "href": "posts/2022-02-07-theory-work/theory-work.html",
    "title": "Theory-Work",
    "section": "",
    "text": "Theoretical work should strive, among other things, “to improve the precision, clarity, and coherence of our ideas” (Martin 2014).\nThis involves cultivating some good habits.\nFor example,\n\nOne important habit for theorizing is just the following: as one writes a period at the end of the sentence, to stop and ask oneself “am I sure this is true? Let me entertain the opposite. Do I have any reason to reject it other than my desire to go forward with my own argument?”\nMartin (2014, pp. 10–11)\n\nSometimes it involves avoiding common traps—e.g., the three “nuance traps” identified described by Healy (2017).\n\nFirst is the ever more detailed, merely empirical description of the world. This is the nuance of the fine-grain. It is a rejection of theory masquerading as increased accuracy. Second is the ever more extensive expansion of some theoretical system in a way that effectively closes it off from rebuttal or disconfirmation by anything in the world. This is the nuance of the conceptual framework. It is an evasion of the demand that a theory be refutable. And third is the insinuation that a sensitivity to nuance is a manifestation of one’s distinctive (often metaphorically expressed and at times seemingly ineffable) ability to grasp and express the richness, texture, and flow of social reality itself. This is the nuance of the connoisseur. It is mostly a species of self-congratulatory symbolic violence\nHealy (2017, pp. 120–21)\n\nRelatedly, sociologists will sometimes stretch their concepts to fit new cases in such a way that brings about the careless redefinition of important terms. For example, Martin (2014) cites Niklas Luhmann saying that “All meaninglessness… has meaning again through its strangeness.” The statement is only true if we accept a new definition for the word meaning. And why should we?\n\nMost generally, whenever we find ourselves rushing to claim that “things outside of any set are themselves in the set” we may be changing our terminology in ways we do not understand. And if we make a habit of it, we’ll end up using meaningless statements.\nMartin (2014, p. 13)\n\nIn short, good theory-work should give us the tools to avoid common traps. It should enable us to tell apart good nuance (e.g. getting important details correct, avoiding oversimplifications) from bad nuance. It should enable us to tell apart good empirical tautologies (e.g., “people try to be good, and good is what ever people say is good”) from bad argumentative tautologies (i.e., assuming at one place what we are claiming to prove at another)."
  },
  {
    "objectID": "posts/2022-02-07-theory-work/theory-work.html#deepities",
    "href": "posts/2022-02-07-theory-work/theory-work.html#deepities",
    "title": "Theory-Work",
    "section": "Deepities",
    "text": "Deepities\nGood theory-work should also prevent us from saying deepities.\n\nA deepity is a proposition that seems both important and true —and profound— but that achieves this effect by being ambiguous. On one reading it is manifestly false, but it would be earth-shaking if it were true; on the other reading it is true but trivial. The unwary listener picks up the glimmer of truth from the second reading, and the devastating importance from the first reading, and thinks, Wow! That’s a deepity.\nDennett (2013, p. 56)\n\n\nDennett’s favorite example is love is just a word.\n\nIn other words, good theory-work should constrain our thinking so that saying stupid things becomes much harder.\n\nConstructionist arguments are not deepities\nDeepities are found all over social science and the humanities, in the sense that ambiguous statements will get rewarded if they seem profound. A lot of constructionist arguments, for example, become deepities when they take the form “\\(X\\) is socially constructed, therefore \\(X\\) is not real”.\nBut most sociologists often take the opposite perspective: “\\(X\\) is real because it’s socially constructed”.1\nThe logic behind constructionist arguments is fairly straightforward.\n\nSocial constructionists about \\(X\\) tend to hold that:\n\n\\(X\\) need not have existed, or need not be at all as it is. \\(X\\), or \\(X\\) as it is at present, is not determined by the nature of things; it is not inevitable.\n\nVery often they go further, and urge that:\n\n\\(X\\) is quite bad as it is.\nWe would be much better off if \\(X\\) were done away with, or at least radically transformed.\n\nHacking (1999, p. 6)\n\nThis kind of argument is not trivial when \\(X\\) is taken for granted —i.e., when \\(X\\) appears to be inevitable.\nThe notion of racecraft is a great example of this kind of argument. It is used to describe how social hierarchy gets projected into “nature” in the form of “race.”\n\nConsider the statement “black Southerners were segregated because of their skin color”—a perfectly natural sentence to the ears of most Americans, who tend to overlook its weird causality.\nFields and Fields (2014, p. 17)\n\nHow can skin color cause segregation? How can skin color cause a stop-and-frisk incident?\nIn a way, this is the opposite of Dennet’s “deepity.” It is a statement that looks trivial and true at first glance. But, upon closer reflection, it has no well-formed meaning. And if it were true, it would have earth shattering consequences to our conception of causality. Which begs the question: why did it originally seem so unremarkable?\nAt their most ambitious, social constructionist arguments are an indictment of folk social theory (or “culture”). They remind us that many arbitrary things come to be seen as “natural” or “inevitable” in the course of everyday life."
  },
  {
    "objectID": "posts/2022-02-07-theory-work/theory-work.html#footnotes",
    "href": "posts/2022-02-07-theory-work/theory-work.html#footnotes",
    "title": "Theory-Work",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is what drove Durkheim to speak of social facts and it’s what drives contemporary sociologists to speak of institutions.↩︎"
  },
  {
    "objectID": "posts/2022-05-12-distributions-in-r/distributions-in-r.html",
    "href": "posts/2022-05-12-distributions-in-r/distributions-in-r.html",
    "title": "Distributions in R",
    "section": "",
    "text": "Set up\nlibrary(tidyverse)\ntheme_set(\n  theme_light(base_family = \"Optima\") + \n  theme(strip.background = element_rect(fill = \"#595959\"))\n)\nR has four built-in forms of working with probability distributions.\nSee ?distributions for a list of common distributions contained in R.\nFor example, to work with a normal distribution we have the following functions:\nA common source of confusion comes from the difference between continuous random variables (e.g. a normal distribution) and discrete random variables (e.g. a binomial distribution).\nThis will all make sense."
  },
  {
    "objectID": "posts/2022-05-12-distributions-in-r/distributions-in-r.html#discrete-distributions",
    "href": "posts/2022-05-12-distributions-in-r/distributions-in-r.html#discrete-distributions",
    "title": "Distributions in R",
    "section": "discrete distributions",
    "text": "discrete distributions\nIn this section we’ll use two distributions as examples.\nThe probability distribution of a binomial random variable comes from adding coin flips (also known as Bernoulli distributions). The Bernoulli distribution has two possible outcomes \\(x = \\{0, 1\\}\\) and one parameter \\(p\\) (which confusingly is also a probability).\nFor example, let’s suppose a coin is loaded and so \\(p = 0.75\\).\nThe probability mass function (PMF) of this Bernoulli distribution is as follows:\n\\[\nf(x) = \\Pr(X = x) = \\begin{cases}\n    0.75 &\\text{if} \\ x = 1 \\\\\\\\\n    0.25 &\\text{if} \\ x = 0 \\\\\\\\\n    0 &\\text{if} \\ x = \\text{anything else}\n\\end{cases}\n\\]\nThe cumulative distribution function (CDF) is as follows:\n\\[\nF(x) = \\Pr(X \\leq x) = \\begin{cases}\n    0 &\\text{if} \\ x &lt; 1 \\\\\n    0.25 &\\text{if} \\ x = 0 \\\\\n    1  &\\text{if} \\ x = 1 \\\\\n    1 &\\text{if} \\ x &gt; 1\n\\end{cases}\n\\]\n\nNote the change in notation from \\(f\\) to \\(F\\).\n\nAny CDF returns the probability that an outcome is less than or equal to \\(x\\). In other words, you’re simply adding up the probability masses for each possible outcome until you reach \\(x\\).\nThe binomial distribution\nAs mentioned earlier, the binomial distribution comes from adding \\(n\\) coin flips. For example, if you throw 3 coins then we have four possible outcomes \\(x = \\{0, 1, 2, 3\\}\\) and two parameters: \\(p\\) and \\(n = 3\\).\nThe probability (mass) function of this binomial distribution is then this:\n\\[\nf(x) = \\Pr(X = x) = \\begin{cases}\n    1 \\ (1 - p)^3 &\\text{if} \\ x = 0 \\\\\n    3 \\ p(1 - p)^2 &\\text{if} \\ x = 1 \\\\\n    3 \\ p^2(1-p) &\\text{if} \\ x = 2 \\\\\n    1\\ p^3 &\\text{if} \\ x = 3 \\\\\n    0 &\\text{if} \\ x = \\text{anything else}\n\\end{cases}\n\\]\nThe 1s and 3s come from counting the number of ways in which \\(x\\) can equal one of these numbers. This is not different from “the garden of forking data” stuff in McElreath (2020, pp. 20–27)\nBut this is not how you’ll see binomial distributions written out in the wild. We need new notation in order to write any binomial distribution, which we get by using the binomial coefficient:\n\\[\n{n \\choose x} = \\frac{n!}{x! (n - x)!}\n\\]\nSo the probability (mass) function of any binomial distribution is then this:\n\\[\nf(x) = \\Pr(X = x) = {n \\choose x} p^x (1-p)^{n-x}\n\\]\nThe cumulative distribution function is as follows:\n\\[\nF(x) = \\Pr(X \\leq x) = \\sum_{i = 0}^x {n \\choose x} p^x (1-p)^{n-x}\n\\]\nFor example, with \\(n = 10\\) and \\(p = 0.5\\), this is how they look:\n\n\nCode\ntibble(x = 0:10) |&gt; \n  mutate(\n    \"Probability Mass Function — dbinom(x, size = 10, p = 1/2)\" = \n      dbinom(x, size = 10, p = 1/2),\n    \"Cumulative Distribution Function — pbinom(x, size = 10, p = 1/2)\" = \n      pbinom(x, size = 10, p = 1/2)\n  ) |&gt;\n  mutate(x = factor(x)) |&gt; \n  pivot_longer(!x, names_to = \"distribution\") |&gt; \n  ggplot(aes(x, value)) + \n  geom_col(width = 1/3) +\n  facet_wrap(~distribution, ncol = 1) + \n  labs(y = NULL)\n\n\n\n\n\nNote that the Bernoulli distribution is now a special case of the binomial distribution in which \\(n = 1\\).\nThis is what’s going on when you use the dbinom and pbinom functions:\nThe Bernoulli PMF is the same as the binomial PMF with \\(n = 1\\)\n\n\nCode\ndbinom(x = c(-2, -1, 0, 1, 2, 3), size = 1, prob = 0.75)\n\n\n[1] 0.00 0.00 0.25 0.75 0.00 0.00\n\n\nBernoulli CDF\n\n\nCode\npbinom(q = c(-2, -1, 0, 1, 2, 3), size = 1, prob = 0.75)\n\n\n[1] 0.00 0.00 0.25 1.00 1.00 1.00\n\n\nBinomial PMF with \\(n=4\\)\n\n\nCode\ndbinom(x = seq(-1, 5), size = 4, prob = 0.75)\n\n\n[1] 0.00000000 0.00390625 0.04687500 0.21093750 0.42187500 0.31640625 0.00000000\n\n\nBinomial CDF with \\(n=4\\)\n\n\nCode\npbinom(q = seq(-1, 5), size = 4, prob = 0.75)\n\n\n[1] 0.00000000 0.00390625 0.05078125 0.26171875 0.68359375 1.00000000 1.00000000\n\n\nNote that because pbinom is just adding different pieces of dbinom together, we could have obtained the same results simply by adding.\nBinomial CDF with \\(n = 4\\)\n\n\nCode\ncumsum(dbinom(x = seq(-1, 5), size = 4, prob = 0.75))\n\n\n[1] 0.00000000 0.00390625 0.05078125 0.26171875 0.68359375 1.00000000 1.00000000\n\n\nDrawing random samples\nrbinom is used to draw samples from dbinom. This makes doing math very easy. For example, suppose we have 12 coin flips—or a binomial distribution with \\(n = 12\\) and \\(p = 0.5\\).\n\n\nCode\ndraws &lt;- rbinom(n = 1e4, size = 12, prob = 0.5) \n\n\nWhat is the probability that \\(x = 7\\)?\n\n\nCode\nmean(draws == 7)  ## approx\n\n\n[1] 0.1879\n\n\nCode\ndbinom(x = 7, size = 12, prob = 0.5)\n\n\n[1] 0.1933594\n\n\nWhat is the probability that \\(x \\leq 8\\)?\n\n\nCode\nmean(draws &lt;= 8)  ## approx\n\n\n[1] 0.9267\n\n\nCode\npbinom(q = 8, size = 12, prob = 0.5)\n\n\n[1] 0.927002\n\n\nWhat is the probability that \\(x\\) is \\(1\\) or \\(4\\) or \\(9\\)?\n\n\nCode\nmean(draws %in% c(1, 4, 9)) ## approx\n\n\n[1] 0.1807\n\n\nCode\nsum(dbinom(x = c(1, 4, 9), size = 12, prob = 0.5))\n\n\n[1] 0.1774902"
  },
  {
    "objectID": "posts/2022-05-12-distributions-in-r/distributions-in-r.html#continuous-distributions",
    "href": "posts/2022-05-12-distributions-in-r/distributions-in-r.html#continuous-distributions",
    "title": "Distributions in R",
    "section": "continuous distributions",
    "text": "continuous distributions\nThe well-known probability (density) distribution for a normal random variable has two parameters \\(\\mu\\) and \\(\\sigma^2\\).\nIt’s ugly:\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\bigg(- \\frac{(x - \\mu)^2}{2 \\sigma^2}\\bigg)\n\\]\nNote that \\(f(x) \\neq \\Pr(X = x)\\).\nBecause \\(x\\) is a real number (that ranges from \\(-\\infty\\) to \\(+\\infty\\)), the probability that \\(x = 1\\) is exactly the same as the probability that \\(x = 0.9999...\\) both are zero.\n\n\nCode\nggplot() + \n  xlim(-5, 5) + \n  geom_function(fun = dnorm) + \n  labs(y = \"density\", x = \"x\")\n\n\n\n\n\nHowever, the cumulative distribution function (CDF) does have the same interpretation. If you add all the possible values until you reach \\(x\\) you get \\(\\Pr(X \\leq x)\\). BUT, because there exists an infinite amount of numbers between \\(-\\infty\\) and \\(x\\), you can’t simply add. You have to integrate.\n\\[\nF(x) = \\Pr(X \\leq x) = \\int_{-\\infty}^x f(x) dx\n\\]\n\n\nCode\nggplot() + \n  xlim(-5, 5) + \n  geom_function(fun = pnorm) + \n  labs(y = \"cumulative probability\", x = \"x\")\n\n\n\n\n\nAssuming \\(\\mu = 0\\) and \\(\\sigma = 2\\), what is the probability that \\(x\\) is less than or equal to zero?\nThe following two chunks of code give the same answer:\n\n\nCode\npnorm(q = 0, mean = 0, sd = 2)\n\n\n[1] 0.5\n\n\nCode\nintegrate(dnorm, lower = -Inf, upper = 0, mean = 0, sd = 2)\n\n\n0.5 with absolute error &lt; 7.3e-07\n\n\nYou can also get an approximate answer by drawing random samples with rnorm.\n\n\nCode\ndraws &lt;- rnorm(1e5, mean = 0, sd = 2)\nmean(draws &lt;= 0) ## approx\n\n\n[1] 0.50201\n\n\nKnowing that the CDF is an integral, we can understand the PDF as the derivative of the CDF. (The derivative of an integral of a function is just the function itself). In other words, a probability density is the rate of change in cumulative probability at \\(x\\). The PDF is the “slope” of the CDF at \\(x\\). This means that if the cumulative probability is increasing rapidly, the density can easily exceed 1. But if we calculate the area under the density function, it will never exceed 1.\n\nSee the “overthinking” section in McElreath (2020, p. 76) for a similar description of this issue.\n\nFor example, compare the PDF and CDF of the exponential distribution. While the CDF eventually converges to 1, the density easily exceeds 1 at some points.\nCDF:\n\n\nCode\nggplot() + \n  xlim(0, 5) + \n  geom_function(fun = function(x) pexp(x, rate = 3)) + \n  labs(y = \"cumulative probability\", x = \"x\")\n\n\n\n\n\nPDF:\n\n\nCode\nggplot() + \n  xlim(0, 5) + \n  geom_function(fun = function(x) dexp(x, rate = 3)) +\n  labs(y = \"density\", x = \"x\")\n\n\n\n\n\nMore examples:\n\n\nCode\ndraws &lt;- rnorm(1e4, mean = 2, sd = 5)\n\n\nWhat is the probability that \\(x = 7\\)?\n\n\nCode\nmean(draws == 7)  ## approx\n\n\n[1] 0\n\n\nWhat is the probability that \\(3 &lt; x &lt; 7\\)?\n\n\nCode\nintegrate(dnorm, mean = 2, sd = 5, lower = 3, upper = 7)\n\n\n0.262085 with absolute error &lt; 2.9e-15\n\n\nCode\nmean(3 &lt; draws & draws &lt; 7)\n\n\n[1] 0.2688\n\n\nWhat is the probability that \\(x \\leq 8\\)?\n\n\nCode\nmean(draws &lt;= 8)  ## approx\n\n\n[1] 0.8836\n\n\nCode\npnorm(8, mean = 2, sd = 5)\n\n\n[1] 0.8849303\n\n\nWhat is the probability that \\(x\\) is NOT between \\(-2\\) and \\(2\\)?\n\n\nCode\n1 - integrate(dnorm, mean = 2, sd = 5, lower = -2, upper = 2)$value\n\n\n[1] 0.7118554\n\n\nCode\nmean(draws &lt; -2 | draws &gt; 2)  ## approx\n\n\n[1] 0.7181"
  },
  {
    "objectID": "posts/2022-05-12-distributions-in-r/distributions-in-r.html#quantile-functions",
    "href": "posts/2022-05-12-distributions-in-r/distributions-in-r.html#quantile-functions",
    "title": "Distributions in R",
    "section": "quantile functions",
    "text": "quantile functions\nThe inverse of a CDF is called a quantile function (\\(Q = F^{-1}\\)).\nThis is where we get stuff like the median:\n\\[\n\\underbrace{Q(0.5)}_\\text{median} = x \\iff \\Pr(X \\leq x) = 0.5\n\\]\nMedian example with the exponential distribution:\n\n\nCode\nqexp(p = 0.5, rate = 3)         ## finding the median\n\n\n[1] 0.2310491\n\n\nCode\npexp(q = 0.2310491, rate = 3)   ## verifying the median\n\n\n[1] 0.5000001\n\n\nCode\ndraws &lt;- rexp(1e5, rate = 3)    ## finding the median using random draws\nquantile(draws, 0.5)            ## approx\n\n\n      50% \n0.2310213"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "What is Computational Social Science?\n\n\n\n\n\n\n\nSocial Science\n\n\nComputation\n\n\n\n\ntl;dr it is a “trading zone” driven by digital technology and associated with a new professional group\n\n\n\n\n\n\nDec 27, 2023\n\n\nandrés castro araújo\n\n\n\n\n\n\n  \n\n\n\n\nDistributions in R\n\n\n\n\n\n\n\nR\n\n\nProbability\n\n\n\n\nA short tutorial.\n\n\n\n\n\n\nMay 12, 2022\n\n\nandrés castro araújo\n\n\n\n\n\n\n  \n\n\n\n\nTheory-Work\n\n\n\n\n\n\n\nTheory\n\n\n\n\nDeepities, social constructionism, and sociological theory.\n\n\n\n\n\n\nFeb 7, 2022\n\n\nandrés castro araújo\n\n\n\n\n\n\n  \n\n\n\n\nRegression to the Mean\n\n\n\n\n\n\n\nStatistics\n\n\nCausality\n\n\n\n\nA visual explanation.\n\n\n\n\n\n\nJan 1, 2022\n\n\nandrés castro araújo\n\n\n\n\n\n\n  \n\n\n\n\nOptimization in R\n\n\n\n\n\n\n\nR\n\n\n\n\nA few different ways to do optimization.\n\n\n\n\n\n\nJan 24, 2021\n\n\nandrés castro araújo\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-01-01-regression-to-the-mean/index.html",
    "href": "posts/2022-01-01-regression-to-the-mean/index.html",
    "title": "Regression to the Mean",
    "section": "",
    "text": "Regression to the mean is one of the greatest parables in statistics. The term originated in 1886, when Francis Galton was studying hereditary patterns in human populations. While he was studying the association between children’s and parent’s heights, he noticed that tall (short) parents tend to have children shorter (taller) than themselves; and that they tended towards a population average.\n\npar-a-ble: A simple story used to illustrate a moral or spiritual lesson, as told by Jesus in the Gospels.\n\n\n\nCode\nHistData::Galton |&gt; \n  ggplot(aes(x = parent, y = child)) + \n  geom_jitter(alpha = 0.5) +\n  ## line of equality\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  ## regression line\n  geom_smooth(method = lm, se = FALSE, color = \"skyblue\") \n\n\n\n\n\n\n\n\n\nThis is the data originally analyzed by Galton himself. The dashed line represents equality, meaning that parents and children have the same height. But Galton noted that children’s heights were actually closer to the blue prediction line.\n\nSee Stigler (2016).\n\nThe key insight is that, whenever randomness is involved, the more extreme outcomes will tend to be followed by more moderate outcomes. Why? Because part of the reason these outcomes are so extreme in the first place is due to randomness. Galton also termed this phenomena “regression to mediocrity”1.\nNowadays, we would look at this same problem using linear regression, which allows us to put the predicted heights of children into a simple formula.\n\n\nCode\nd &lt;- HistData::Galton |&gt; \n  mutate(parent_centered = parent - mean(parent))\n\nOLS &lt;- lm(child ~ parent_centered, data = d) ## scaling\n\ncoefficients(OLS)\n\n\n    (Intercept) parent_centered \n     68.0884698       0.6462906 \n\n\nNote that the heights of parents have been centered in order to produce an intercept that corresponds to the population average (or “mediocrity”).\n\\[\n\\widehat h_\\text{child} =  \\underbrace{68.09}_\\text{mediocrity} + 0.65 \\times \\widetilde h_\\text{parent}\n\\]\nAt first encounter, some people will interpret this as meaning that regression to the mean implies that heights will become “more average” over time. Thus, a parent who is 5 inches above average is predicted to have children 3.2 inches taller than average, who in turn are predicted to have children 2.1 inches taller than average, and so on. This very common misunderstanding arises from neglecting the error term when thinking about future observations.\n\\[\nh_\\text{child} = \\widehat h_\\text{child} + \\text{error}\n\\]\nIn Gelman et al’s words:\n\nThe point predictions regress toward the mean—that’s the coefficient less than 1—and this reduces variation. At the same time, though, the error in the model—the imperfection of the prediction—adds variation, just enough to keep the total variation in height roughly constant from one generation to the next.\nRegression to the mean thus will always arise in some form whenever predictions are imperfect in a stable environment. The imperfection of the prediction induces variation, and regression in the point prediction is required in order to keep the total variation constant.\nGelman et al. (2020, p. 88)\n\nThe following graphs show both ways of interpreting regression to the mean, based on how the data could look like across 12 generations:\n\n\nCode\nOLS &lt;- lm(child ~ parent, data = HistData::Galton)\nintercept &lt;- coefficients(OLS)[[1]]\nslope &lt;- coefficients(OLS)[[2]]\nsigma &lt;- sd(residuals(OLS))\n\ng0 &lt;- HistData::Galton$parent # first ancestors\n\n## Simulation\n\nreg_naive &lt;- function(x, ...) {\n  x * slope + intercept\n}\n\nreg_correct &lt;- function(x, ...) {\n  rnorm(n = length(x), mean = x * slope + intercept, sd = sigma)\n}\n\nn &lt;- 12 # number of generations\n\ns_naive &lt;- accumulate(1:n, reg_naive, .init = g0) \ns_correct &lt;- accumulate(1:n, reg_correct, .init = g0)\nnames(s_correct) &lt;- names(s_naive) &lt;- 0:n\n\ndf_naive &lt;- as_tibble(s_naive) |&gt; \n  mutate(ancestor = row_number()) |&gt; \n  pivot_longer(cols = !ancestor, names_to = \"generation\", values_to = \"height\") |&gt; \n  mutate(simulation = \"naive interpretation\", generation = as.integer(generation))\n\ndf_correct &lt;- as_tibble(s_correct) |&gt; \n  mutate(ancestor = row_number()) |&gt; \n  pivot_longer(cols = !ancestor, names_to = \"generation\", values_to = \"height\") |&gt; \n  mutate(simulation = \"correct interpretation\", generation = as.integer(generation))\n\nbind_rows(df_correct, df_naive) |&gt; \n  ggplot(aes(x = generation, y = height, color = ancestor, group = ancestor)) +\n  geom_line(show.legend = FALSE, alpha = 1/10) +\n  facet_wrap(~ simulation, ncol = 1, scales = \"free_y\") + \n  labs(y = \"height (inches)\") + \n  scale_color_viridis_c() +\n  scale_x_continuous(breaks = 0:12) +\n  theme(strip.text.x = element_text(size = 14)) \n\n\n\n\n\n\n\n\n\n\nThis very simple simulation assumes that the population average remains constant over time, which is obviously false when you consider changes in public health, such as nutrition.\nAs a general rule, nothing in the social and biological sciences remains constant over very long periods of time."
  },
  {
    "objectID": "posts/2022-01-01-regression-to-the-mean/index.html#origin-story",
    "href": "posts/2022-01-01-regression-to-the-mean/index.html#origin-story",
    "title": "Regression to the Mean",
    "section": "",
    "text": "Regression to the mean is one of the greatest parables in statistics. The term originated in 1886, when Francis Galton was studying hereditary patterns in human populations. While he was studying the association between children’s and parent’s heights, he noticed that tall (short) parents tend to have children shorter (taller) than themselves; and that they tended towards a population average.\n\npar-a-ble: A simple story used to illustrate a moral or spiritual lesson, as told by Jesus in the Gospels.\n\n\n\nCode\nHistData::Galton |&gt; \n  ggplot(aes(x = parent, y = child)) + \n  geom_jitter(alpha = 0.5) +\n  ## line of equality\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  ## regression line\n  geom_smooth(method = lm, se = FALSE, color = \"skyblue\") \n\n\n\n\n\n\n\n\n\nThis is the data originally analyzed by Galton himself. The dashed line represents equality, meaning that parents and children have the same height. But Galton noted that children’s heights were actually closer to the blue prediction line.\n\nSee Stigler (2016).\n\nThe key insight is that, whenever randomness is involved, the more extreme outcomes will tend to be followed by more moderate outcomes. Why? Because part of the reason these outcomes are so extreme in the first place is due to randomness. Galton also termed this phenomena “regression to mediocrity”1.\nNowadays, we would look at this same problem using linear regression, which allows us to put the predicted heights of children into a simple formula.\n\n\nCode\nd &lt;- HistData::Galton |&gt; \n  mutate(parent_centered = parent - mean(parent))\n\nOLS &lt;- lm(child ~ parent_centered, data = d) ## scaling\n\ncoefficients(OLS)\n\n\n    (Intercept) parent_centered \n     68.0884698       0.6462906 \n\n\nNote that the heights of parents have been centered in order to produce an intercept that corresponds to the population average (or “mediocrity”).\n\\[\n\\widehat h_\\text{child} =  \\underbrace{68.09}_\\text{mediocrity} + 0.65 \\times \\widetilde h_\\text{parent}\n\\]\nAt first encounter, some people will interpret this as meaning that regression to the mean implies that heights will become “more average” over time. Thus, a parent who is 5 inches above average is predicted to have children 3.2 inches taller than average, who in turn are predicted to have children 2.1 inches taller than average, and so on. This very common misunderstanding arises from neglecting the error term when thinking about future observations.\n\\[\nh_\\text{child} = \\widehat h_\\text{child} + \\text{error}\n\\]\nIn Gelman et al’s words:\n\nThe point predictions regress toward the mean—that’s the coefficient less than 1—and this reduces variation. At the same time, though, the error in the model—the imperfection of the prediction—adds variation, just enough to keep the total variation in height roughly constant from one generation to the next.\nRegression to the mean thus will always arise in some form whenever predictions are imperfect in a stable environment. The imperfection of the prediction induces variation, and regression in the point prediction is required in order to keep the total variation constant.\nGelman et al. (2020, p. 88)\n\nThe following graphs show both ways of interpreting regression to the mean, based on how the data could look like across 12 generations:\n\n\nCode\nOLS &lt;- lm(child ~ parent, data = HistData::Galton)\nintercept &lt;- coefficients(OLS)[[1]]\nslope &lt;- coefficients(OLS)[[2]]\nsigma &lt;- sd(residuals(OLS))\n\ng0 &lt;- HistData::Galton$parent # first ancestors\n\n## Simulation\n\nreg_naive &lt;- function(x, ...) {\n  x * slope + intercept\n}\n\nreg_correct &lt;- function(x, ...) {\n  rnorm(n = length(x), mean = x * slope + intercept, sd = sigma)\n}\n\nn &lt;- 12 # number of generations\n\ns_naive &lt;- accumulate(1:n, reg_naive, .init = g0) \ns_correct &lt;- accumulate(1:n, reg_correct, .init = g0)\nnames(s_correct) &lt;- names(s_naive) &lt;- 0:n\n\ndf_naive &lt;- as_tibble(s_naive) |&gt; \n  mutate(ancestor = row_number()) |&gt; \n  pivot_longer(cols = !ancestor, names_to = \"generation\", values_to = \"height\") |&gt; \n  mutate(simulation = \"naive interpretation\", generation = as.integer(generation))\n\ndf_correct &lt;- as_tibble(s_correct) |&gt; \n  mutate(ancestor = row_number()) |&gt; \n  pivot_longer(cols = !ancestor, names_to = \"generation\", values_to = \"height\") |&gt; \n  mutate(simulation = \"correct interpretation\", generation = as.integer(generation))\n\nbind_rows(df_correct, df_naive) |&gt; \n  ggplot(aes(x = generation, y = height, color = ancestor, group = ancestor)) +\n  geom_line(show.legend = FALSE, alpha = 1/10) +\n  facet_wrap(~ simulation, ncol = 1, scales = \"free_y\") + \n  labs(y = \"height (inches)\") + \n  scale_color_viridis_c() +\n  scale_x_continuous(breaks = 0:12) +\n  theme(strip.text.x = element_text(size = 14)) \n\n\n\n\n\n\n\n\n\n\nThis very simple simulation assumes that the population average remains constant over time, which is obviously false when you consider changes in public health, such as nutrition.\nAs a general rule, nothing in the social and biological sciences remains constant over very long periods of time."
  },
  {
    "objectID": "posts/2022-01-01-regression-to-the-mean/index.html#the-lesson",
    "href": "posts/2022-01-01-regression-to-the-mean/index.html#the-lesson",
    "title": "Regression to the Mean",
    "section": "The lesson",
    "text": "The lesson\nRegression to the mean will people to see causality where there is none.\nAs mentioned earlier, this phenomena shows up all the time wherever randomness (or “luck”) is expected to play a role. Thus, we see it in everything from sports to public policy.2 This is a problem because human beings are inclined to find amazing patterns when they look at randomness. Obviously, this is mostly harmless when, for example, the source of randomness is clouds in the sky and the patterns we find take the form of “a cow” or “a face.”3 But in other settings, this same phenomena can become worrisome.\nAn exercise in simulation4\nA very famous real-world example is contained in an article titled “On the psychology of prediction” Tversky & Kahneman, 1973):\n\nThe instructors in a flight school adopted a policy of consistent positive reinforcement recommended by psychologists. They verbally reinforced each successful execution of a flight maneuver. After some experience with this training approach, the instructors claimed that contrary to psychological doctrine, high praise for good execution of complex maneuvers typically results in a decrement of performance on the next try. What should the psychologist say in response?\n\nTo answer this question we simulate data from 500 pilots, each of whom performs two maneuvers, and with each maneuver scored continuously on a 0-10 scale. Each pilot has a “true ability” that is unchanged during the two tasks, and the score for each test is equal to this true ability plus an independent error. Pilots get praised when they score higher than 7 during the first maneuver, and receive negative reinforcement when they score lower than 3.\nThe connection between this example and Galton’s study is not obvious at first glance. Here, each pilot’s score exhibits regression to her “true ability”, in the sense that we expect “luck” to play an important role. As before, we don’t really expect these “true abilities” to remain constant over time. In fact that would defeat the entire purpose of flight school. But we do expect these underlying abilities to change very slowly.\nNote that we make sure, by design, that reinforcement has no effect on performance on the second task.\n\n\nCode\nN &lt;- 500\ntrue_abilities &lt;- rnorm(N, 0, 1)\n# mapping the true abilities + random noise to a [0,10] scale\ntask1 &lt;- plogis(true_abilities + rnorm(N, 0, 0.5)) * 10\ntask2 &lt;- plogis(true_abilities + rnorm(N, 0, 0.5)) * 10\n\ndf &lt;- tibble(true_abilities, task1, task2) |&gt; \n  mutate(reinforcement = case_when(\n    task1 &lt; 3 ~ \"negative\",\n    task1 &gt; 7 ~ \"positive\",\n    TRUE ~ \"neutral\")\n  )\n\ndf |&gt; \n  ggplot(aes(x = task1, y = task2, color = reinforcement)) +\n  geom_point() +\n  scale_color_manual(values = c(\"pink\", \"grey\", \"skyblue\")) +\n  scale_x_continuous(breaks = seq(2, 10, 2)) +\n  scale_y_continuous(breaks = seq(2, 10, 2))\n\n\n\n\n\nWe can now compute the average change in scores for each group of pilots, which the instructors interpret to be the causal effect of reinforcement.\n\n\nCode\ndf |&gt; \n  mutate(change = task2 - task1) |&gt; \n  group_by(reinforcement) |&gt; \n  summarise(\n    effect = mean(change),\n    se = sd(change) / sqrt(n())\n    ) |&gt; \n  ggplot(aes(x = reinforcement, y = effect)) +\n  geom_pointrange(aes(\n    ymin = effect + qnorm(p = 0.025)*se,\n    ymax = effect + qnorm(p = 0.975)*se\n    )) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  coord_flip()\n\n\n\n\n\nNotice that, on average, the pilots who were praised did worse on the second task, whereas the pilots who received negative reinforcement did better. But we know that such effect doesn’t exist. And we know this because we created these data so that task 1 and task 2 were unrelated. The “causal pattern” we have just observed is a consequence of the noise in the data. Pilots who scored very well on task 1 are likely to have a higher skill and also to have been somewhat lucky. Thus, it makes sense that they perform slightly worse on task 2.\nThis is how Tversky and Kahneman (1982) explain it:\n\nRegression is inevitable in flight maneuvers because performance is not perfectly reliable and progress between successive maneuvers is slow. Hence, pilots who did exceptionally well on one trial are likely to deteriorate on the next, regardless of the instructors’ reaction to the initial success. The experienced flight instructors actually discovered the regression but attributed it to the detrimental effect of positive reinforcement. This true story illustrates a saddening aspect of the human condition. We normally reinforce others when their behavior is good and punish them when their behavior is bad. By regression alone, therefore, they are most likely to improve after being punished and most likely to deteriorate after being rewarded. Consequently, we are exposed to a lifetime schedule in which we are most often rewarded for punishing others, and punished for rewarding.\nQuoted in Gelman et al. (2020, p. 90)"
  },
  {
    "objectID": "posts/2022-01-01-regression-to-the-mean/index.html#footnotes",
    "href": "posts/2022-01-01-regression-to-the-mean/index.html#footnotes",
    "title": "Regression to the Mean",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHistorically, there have been two kinds moral judgments regarding averages and normal distributions (Hacking 1990): (1) the Quetelet-Durkheim conception of the normal as the right and the good; and (2) Galton’s notion of the normal as the mediocre, and in need of improvement.↩︎\nThis also seems to be the reason why so many bands appear to suffer from “second album syndrome” or “sophomore slump”.↩︎\nSee: pareidolia.↩︎\nThis my solution to exercise 6.8 in Regression and Other Stories.↩︎"
  },
  {
    "objectID": "posts/2021-01-24-optimization-in-r/index.html",
    "href": "posts/2021-01-24-optimization-in-r/index.html",
    "title": "Optimization in R",
    "section": "",
    "text": "Finding the peak of parabola\n\\[y = 15 + 10x - 2x^2\\]\nFirst we write this statement as an R function.\nCode\nparabola &lt;- function(x) 15 + 10*x - 2*x^2\nThen we can visualize it using curve() or ggplot2.\nCode\nlibrary(ggplot2)\ntheme_set(theme_light(base_family = \"Optima\"))\n\ng &lt;- ggplot() + \n  geom_function(fun = parabola) +\n  xlim(0, 5) +\n  labs(x = \"x\", y = \"f(x)\")\n\ng\nThen we call optimize(), which takes the function as its first argument, the interval as its second, and an optional argument indicating whether or not you are searching for the function’s maximum (minimize is the default).\nCode\nout &lt;- optimize(parabola, interval = c(-100, 100), maximum = TRUE)\nout\n\n\n$maximum\n[1] 2.5\n\n$objective\n[1] 27.5\n\n\nCode\ng + geom_vline(xintercept = out$maximum, linetype = \"dashed\") +\n    geom_hline(yintercept = out$objective, linetype = \"dashed\")\nThere are many more ways of using optimization in R. For example, if you want to find the maximum of a function with many parameters you can use optim().\nCode\nopt &lt;- optim(\n  par = 99, ## initial values; use c(...) to do it with many parameters\n  fn = parabola, \n  method = \"BFGS\",\n  # this next line is critical: \n  # it tells R to maximize rather than minimize\n  control = list(fnscale = -1)\n)\n\nopt\n\n\n$par\n[1] 2.5\n\n$value\n[1] 27.5\n\n$counts\nfunction gradient \n       6        3 \n\n$convergence\n[1] 0\n\n$message\nNULL"
  },
  {
    "objectID": "posts/2021-01-24-optimization-in-r/index.html#logistic-regression",
    "href": "posts/2021-01-24-optimization-in-r/index.html#logistic-regression",
    "title": "Optimization in R",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nIn statistics we usually try to find the maximum of likelihood functions in order to fit regression models.\nFor example1, a simple logistic regression can be fit by doing the following:\n\n\nCode\n## The dataset\ndata(\"wells\", package = \"rstanarm\")\nstr(wells)\n\n\n'data.frame':   3020 obs. of  5 variables:\n $ switch : int  1 1 0 1 1 1 1 1 1 1 ...\n $ arsenic: num  2.36 0.71 2.07 1.15 1.1 3.9 2.97 3.24 3.28 2.52 ...\n $ dist   : num  16.8 47.3 21 21.5 40.9 ...\n $ assoc  : int  0 0 0 0 1 1 1 0 1 1 ...\n $ educ   : int  0 0 10 12 14 9 4 10 0 0 ...\n\n\nCode\n## The model\nf &lt;- formula(switch ~ dist + arsenic + dist:arsenic)\n\n## The design matrix\nX &lt;- model.matrix(f, data = wells)\ndim(X)\n\n\n[1] 3020    4\n\n\nCode\n## The outcome variable\ny &lt;- wells$switch\n\n## The log-likelihood function\nlog_likelihood &lt;- function(beta, outcome, dmat) {\n  \n  linpred &lt;- dmat %*% beta ## the linear predictor\n  p &lt;- plogis(linpred)     ## the link function\n  \n  sum(dbinom(outcome, size = 1, prob = p, log = TRUE))  ## the log-likelihood\n}\n\n## The maximum likelihood estimate (MLE)\nopt &lt;- optim(\n  par = rep(0, ncol(X)), ## initial values are all 0's\n  fn = log_likelihood, method = \"BFGS\",\n  outcome = y, dmat = X,\n  # this next line is critical: \n  # it tells R to maximize rather than minimize\n  control = list(fnscale = -1)\n)\n\nnames(opt$par) &lt;- colnames(X)\nopt$par\n\n\n (Intercept)         dist      arsenic dist:arsenic \n-0.147139626 -0.005811349  0.555574583 -0.001768792 \n\n\nWe can compare this to the outcome given by R’s glm() function:\n\n\nCode\nfit &lt;- glm(f, data = wells, family = binomial(link = \"logit\"))\ncoefficients(fit)\n\n\n (Intercept)         dist      arsenic dist:arsenic \n-0.147868069 -0.005772178  0.555976744 -0.001789060"
  },
  {
    "objectID": "posts/2021-01-24-optimization-in-r/index.html#stan",
    "href": "posts/2021-01-24-optimization-in-r/index.html#stan",
    "title": "Optimization in R",
    "section": "Stan",
    "text": "Stan\nUsers of Stan should know that it can be used for optimization as well.\n\n\nCode\nlibrary(cmdstanr)\nmcmc_optim &lt;- cmdstan_model(\"simple_parabola.stan\")\nmcmc_optim$print()\n\n\nparameters {\n  real&lt;lower=0&gt; x; // easily does constrained optimization\n}\n\nmodel {\n  target += 15 + 10*x - 2*x^2;\n}\n\n\nCode\nfit &lt;- mcmc_optim$optimize()\n\n\nInitial log joint probability = 24.7808 \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes  \n       6          27.5   8.46977e-05   8.30169e-06           1           1        8    \nOptimization terminated normally:  \n  Convergence detected: relative gradient magnitude is below tolerance \nFinished in  0.2 seconds.\n\n\nCode\nfit\n\n\n variable estimate\n     lp__    27.50\n     x        2.50"
  },
  {
    "objectID": "posts/2021-01-24-optimization-in-r/index.html#footnotes",
    "href": "posts/2021-01-24-optimization-in-r/index.html#footnotes",
    "title": "Optimization in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe data: «A survey of 3020 residents in a small area of Bangladesh suffering from arsenic contamination of groundwater. Respondents with elevated arsenic levels in their wells had been encouraged to switch their water source to a safe public or private well in the nearby area and the survey was conducted several years later to learn which of the affected residents had switched wells»↩︎"
  }
]