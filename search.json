[
  {
    "objectID": "posts/2024-01-02-what-is-usa-sociology/index.html",
    "href": "posts/2024-01-02-what-is-usa-sociology/index.html",
    "title": "What is Sociology in the USA?",
    "section": "",
    "text": "Sociologists in the USA like describing sociology as the most heterogeneous social science of all, which is\n\n…perhaps another way of saying that it has been less successful at institutionalizing itself as a discipline than its close relatives. Unlike economics, it does not have a core kit of analytical tools and models codified in textbooks and widely accepted as legitimate both inside and outside the field… Unlike political science, on the other hand, sociology does not have a well-defined empirical core to unify it, either.\nHealy (2012, p. 88)\n\nThere are two ways of looking at this.\nGlass Half-Full\nAndrew Abbott (2001) claims sociology’s defining characteristic is “the fact that the discipline is not very good at excluding things from itself.” Some see this as something good because it turns sociology into some miniature version of all social science. In this view, the lack of “internal cohesion” in the discipline is interpreted as the cost we’ve paid in exchange for occupying a central place in the social science landscape (Moody and Light 2006).\nMany celebrate the lack of a disciplinary core as providing some unique form of academic freedom.\nThis is the reason why many people claim they haven chosen to become sociologists.\n\nI chose sociology because more than any other social science sociology would let me do what I pleased. If I went into sociology, I wouldn’t have to make up my mind what to do.\n—Andrew Abbott\nWhy, then, did I choose sociology as an academic home?\nOf all the available social sciences, sociology seemed to me to be the least disciplinary; it had the fuzziest boundaries. But even more significantly, sociology has valued its own marginal traditions in a way that other social sciences don’t.\n—Erik Olin Wright\n\nGlass Half-Empty\nOthers are less optimistic.\nPresumably, we are not as interdisciplinary as we would like to believe.\n\n…most sociologists don’t really get interdisciplinarity. Whether we acknowledge it or not, most of us have internalized a sociological supremacy that makes us believe our field’s insights are more important, more complete, more nuanced than those of other scientists (Healy 2017). This cultural background of intellectual superiority helps create what Lizardo (2014) called the “Comtean schema”—the implicit belief that all proper interdisciplinary research should take the institutional form of a subfield of sociology.\nIn a brilliant insight, Lizardo noted that sociologists create virtual “avatars” of other disciplines within sociology instead of working with their real-world counterparts. That is, rather than engage with economists, we create “economic sociology”; rather than engage with political science, we have “political sociology”; rather than engage with cultural evolution or cognitive science, we invent “the sociology of culture and cognition,” and so on. This fools us into thinking that we’re being interdisciplinary when, in reality, “[t]hese subdisciplinary avatars have been created by sociologists for sociological consumption” (Lizardo 2014:985).\nVaisey (2021, p. 1298)"
  },
  {
    "objectID": "posts/2024-01-02-what-is-usa-sociology/index.html#background",
    "href": "posts/2024-01-02-what-is-usa-sociology/index.html#background",
    "title": "What is Sociology in the USA?",
    "section": "",
    "text": "Sociologists in the USA like describing sociology as the most heterogeneous social science of all, which is\n\n…perhaps another way of saying that it has been less successful at institutionalizing itself as a discipline than its close relatives. Unlike economics, it does not have a core kit of analytical tools and models codified in textbooks and widely accepted as legitimate both inside and outside the field… Unlike political science, on the other hand, sociology does not have a well-defined empirical core to unify it, either.\nHealy (2012, p. 88)\n\nThere are two ways of looking at this.\nGlass Half-Full\nAndrew Abbott (2001) claims sociology’s defining characteristic is “the fact that the discipline is not very good at excluding things from itself.” Some see this as something good because it turns sociology into some miniature version of all social science. In this view, the lack of “internal cohesion” in the discipline is interpreted as the cost we’ve paid in exchange for occupying a central place in the social science landscape (Moody and Light 2006).\nMany celebrate the lack of a disciplinary core as providing some unique form of academic freedom.\nThis is the reason why many people claim they haven chosen to become sociologists.\n\nI chose sociology because more than any other social science sociology would let me do what I pleased. If I went into sociology, I wouldn’t have to make up my mind what to do.\n—Andrew Abbott\nWhy, then, did I choose sociology as an academic home?\nOf all the available social sciences, sociology seemed to me to be the least disciplinary; it had the fuzziest boundaries. But even more significantly, sociology has valued its own marginal traditions in a way that other social sciences don’t.\n—Erik Olin Wright\n\nGlass Half-Empty\nOthers are less optimistic.\nPresumably, we are not as interdisciplinary as we would like to believe.\n\n…most sociologists don’t really get interdisciplinarity. Whether we acknowledge it or not, most of us have internalized a sociological supremacy that makes us believe our field’s insights are more important, more complete, more nuanced than those of other scientists (Healy 2017). This cultural background of intellectual superiority helps create what Lizardo (2014) called the “Comtean schema”—the implicit belief that all proper interdisciplinary research should take the institutional form of a subfield of sociology.\nIn a brilliant insight, Lizardo noted that sociologists create virtual “avatars” of other disciplines within sociology instead of working with their real-world counterparts. That is, rather than engage with economists, we create “economic sociology”; rather than engage with political science, we have “political sociology”; rather than engage with cultural evolution or cognitive science, we invent “the sociology of culture and cognition,” and so on. This fools us into thinking that we’re being interdisciplinary when, in reality, “[t]hese subdisciplinary avatars have been created by sociologists for sociological consumption” (Lizardo 2014:985).\nVaisey (2021, p. 1298)"
  },
  {
    "objectID": "posts/2024-01-02-what-is-usa-sociology/index.html#data",
    "href": "posts/2024-01-02-what-is-usa-sociology/index.html#data",
    "title": "What is Sociology in the USA?",
    "section": "Data",
    "text": "Data\nNote. The visualization provided below is provisional. I will need to email someone at the ASA and politely ask for permission to use the full directory.\nWhen a sociologist joins the American Sociological Association (ASA), they have the options to choose up to four “areas of interest” for everyone else to see. For example, I chose (1) Theory; (2) Quantitative Methodology; (3) Organizations, Formal and Complex; and (4) Law and Society. And I would probably have chosen something different on a different day.\nSo, I created a dataset containing the areas of interest of over 1,000 individuals associated with 20 sociology programs in the USA. This dataset is provisional, but see Table 1 in case you are curious about the sample.\nEach of these individuals chose anywhere between one and four areas of interest. Figure 1 highlights the ten largest areas of interest in blue. The largest is Stratification/ Mobility, which gives credence to the view that inequality defines the subject-matter of sociology. Among the top ten we also find what some may describe as sub-disciplinary avatars—Cultural Sociology, Economic Sociology, and Political Sociology—which gives credence to the view that sociology is a miniature version of all social science. Furthermore, the view that sociology is mostly concerned with race, class, and gender is also well represented in this top ten.\nNote that some areas of interest like Demography and Medical Sociology are very well represented but don’t seem to\n\n\n\n\n\n\n\n\nFigure 1: Areas of interest in the dataset. The largest 10 areas are highlighted in blue.\n\n\n\n\n\nHowever, this doesn’t provide us with any information about the “structure” of the sociological areas of interest—i.e., the pattern of connections which some may describe in terms of coherency (or lack thereof).\nThis is what the next section is all about."
  },
  {
    "objectID": "posts/2024-01-02-what-is-usa-sociology/index.html#two-mode-networks",
    "href": "posts/2024-01-02-what-is-usa-sociology/index.html#two-mode-networks",
    "title": "What is Sociology in the USA?",
    "section": "Two-Mode Networks",
    "text": "Two-Mode Networks\nThe direct ties between individuals and areas can be transmogrified—via simple matrix multiplication —into indirect ties among areas (Breiger 1974; Agneessens and Everett 2013). This idea should be familiar to everyone who chose Social Networks as area of interest.\n\ntransmogrification is not the “technical” term for this\n\nTo do this, we arrange the individuals and their corresponding areas in matrix form. Here, we have a matrix \\(\\mathbf X\\) with 1112 rows (one for each individual) and 61 columns (one for each area). Each cell \\(x_{ij}\\) is either a \\(1\\) or a \\(0\\). Then we transform \\(\\mathbf X\\) into a new adjacency matrix \\(\\mathbf A\\) with each cell \\(a_{ij}\\) corresponding to the number of “shared” or “intersecting” individuals between areas \\(i\\) and \\(j\\).\n\\[\n\\mathbf{A} = \\mathbf{X}^\\top \\mathbf{X}\n\\tag{1}\\]\nThe diagonal of \\(\\mathbf A\\) contains the total number of individuals that chose each area, which corresponds to the same numbers in Figure 1.1\nUnfortunately, the resulting network, is very dense. It takes only one single individual with idiosyncratic tastes to create ties among four otherwise disconnected areas. As a result, the “structure” is one in which every area is seemingly connected to every other area.2\nBecause of this issue, researchers have looked for ways to “trim” uninformative ties. The intuition is straightforward. If the observed number of shared individuals between two areas ( \\(a_{ij}\\) ) exceeds the number we would expect to see in a random network (the null model), then we draw a tie between area \\(i\\) and area \\(j\\). In other words, we do a “significance test” for each tie in the network depicted by \\(\\mathbf A\\).\nIf the links between individuals and areas of interest were random, the resulting network would consist entirely of isolated nodes. This is what would happen if there was no disciplinary structure."
  },
  {
    "objectID": "posts/2024-01-02-what-is-usa-sociology/index.html#results",
    "href": "posts/2024-01-02-what-is-usa-sociology/index.html#results",
    "title": "What is Sociology in the USA?",
    "section": "Results",
    "text": "Results\nThe results of this procedure are shown in Figure 2, which uses a significance level of 0.01 to draw ties between areas.\nI used Zachary Neal’s (2022) backbone package to do all this.3\nAs we may have expected, there is a strong overlap between Stratification/ Mobility and Education; Demography and Family; Medical Sociology and Mental Health; Migration/Immigration and Latina/o Sociology; and so on.\nThe strong overlap between Cultural Sociology and so-called Theory deserves its own blog post.\nNote. All isolate nodes have been removed. A bigger sample of individuals and their chosen areas would obviously result in more connected areas.\n\n\n\n\n\n\n\n\nFigure 2: “Strong” Ties ( α = 0.01 )\n\n\n\n\nFigure 3 uses a significance level of \\(0.05\\) to draw the ties between areas, which means we get to extract more of them.\nThe result is that we get to see more “structure.”\nFor example, the heterogeneity within Economic Sociology may be more easily understood by looking at adjacent areas: Social Networks, Political Economy, Science and Technology, and Organizations, Formal and Complex. The same might be said for other areas that look like “hubs” (e.g., Stratification/ Mobility, Political Sociology).\nFuthermore, perhaps we may hypothesize that “triangles”—like the overlap between Medical Sociology, Aging/ Social Gerontology, and Demography—are formed by somewhat distinct underlying groups of sociologists.\nAlso, note that Quantitative Methodology now appears connected to three areas (Stratification/ Mobility, Demography, and Mathematical Sociology), but Qualitative Methodology is nowhere to be seen yet. This does not mean that one is more important than the other. It just means that the overlap between Quantitative Methodology and those three areas is stronger than what we would expect by chance; whereas the overlap between Qualitative Methodology and other areas is more compatible with a “null model” of random ties between areas.4\n\n\n\n\n\n\n\n\nFigure 3: “Strongish” Ties ( α = 0.05 )\n\n\n\n\nFinally, Figure 4 uses a significance level of \\(0.06\\) to draw ties, just enough for connections between smaller areas to start showing up (e.g., Marxist Sociology, Alcohol and Drugs). Religion is now connected to Cultural Sociology\nPlease keep in mind that this does not mean that these connections are somehow “weaker” than the other ones in any substantive way. Hypothesis testing is extremely dependent on sample size.\n\n\n\n\n\n\n\n\nFigure 4: “Strongish” Ties ( α = 0.06 )\n\n\n\n\nThat’s all for now."
  },
  {
    "objectID": "posts/2024-01-02-what-is-usa-sociology/index.html#sample",
    "href": "posts/2024-01-02-what-is-usa-sociology/index.html#sample",
    "title": "What is Sociology in the USA?",
    "section": "Sample",
    "text": "Sample\n\n\n\n\n\n\n\n\n\n\n\nUniversities and Individuals\n\n\n\nn\n%\n\n\n\n\nNorthwestern University\n84\n7.55%\n\n\nUniversity of Michigan-Ann Arbor\n76\n6.83%\n\n\nUniversity of Chicago\n72\n6.47%\n\n\nHarvard University\n69\n6.21%\n\n\nUniversity of Wisconsin-Madison\n68\n6.12%\n\n\nStanford University\n66\n5.94%\n\n\nUniversity of Pennsylvania\n66\n5.94%\n\n\nCornell University\n65\n5.85%\n\n\nBrown University\n62\n5.58%\n\n\nNew York University\n61\n5.49%\n\n\nPrinceton University\n59\n5.31%\n\n\nColumbia University\n47\n4.23%\n\n\nUC Berkeley\n46\n4.14%\n\n\nOhio State University\n45\n4.05%\n\n\nDuke University\n42\n3.78%\n\n\nYale University\n42\n3.78%\n\n\nUniversity of California-Los Angeles\n41\n3.69%\n\n\nUniversity of North Carolina-Chapel Hill\n36\n3.24%\n\n\nIndiana University-Bloomington\n33\n2.97%\n\n\nUniversity of Texas-Austin\n32\n2.88%\n\n\nTotal\n1,112\n100%\n\n\n\n\n\n\n\n\nTable 1: Number of individuals extracted from the ASA member directory"
  },
  {
    "objectID": "posts/2024-01-02-what-is-usa-sociology/index.html#citation",
    "href": "posts/2024-01-02-what-is-usa-sociology/index.html#citation",
    "title": "What is Sociology in the USA?",
    "section": "Citation",
    "text": "Citation\nThe backbone package automatically outputs the following suggested text:\n\nFigure 2:\nWe used the backbone package for R (v2.1.2; Neal, 2022) to extract the unweighted backbone of a weighted and undirected unipartite network containing 61 nodes. An edge was retained in the backbone if its weight was statistically significant (alpha = 0.01) using the disparity filter (Serrano et al., 2009). This reduced the number of edges by 99%, and reduced the number of connected nodes by 68.9%.\nFigure 3:\nWe used the backbone package for R (v2.1.2; Neal, 2022) to extract the unweighted backbone of a weighted and undirected unipartite network containing 61 nodes. An edge was retained in the backbone if its weight was statistically significant (alpha = 0.05) using the disparity filter (Serrano et al., 2009). This reduced the number of edges by 96.3%, and reduced the number of connected nodes by 34.4%.\nFigure 4:\nWe used the backbone package for R (v2.1.2; Neal, 2022) to extract the unweighted backbone of a weighted and undirected unipartite network containing 61 nodes. An edge was retained in the backbone if its weight was statistically significant (alpha = 0.06) using the disparity filter (Serrano et al., 2009). This reduced the number of edges by 94.9%, and reduced the number of connected nodes by 21.3%.\nNeal, Z. P. 2022. backbone: An R Package to Extract Network Backbones. PLOS ONE, 17, e0269137. https://doi.org/10.1371/journal.pone.0269137\nSerrano, M. A., Boguna, M., & Vespignani, A. (2009). Extracting the multiscale backbone of complex weighted networks. Proceedings of the National Academy of Sciences, 106(16), 6483-6488. https://doi.org/10.1073/pnas.0808904106"
  },
  {
    "objectID": "posts/2024-01-02-what-is-usa-sociology/index.html#footnotes",
    "href": "posts/2024-01-02-what-is-usa-sociology/index.html#footnotes",
    "title": "What is Sociology in the USA?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis new matrix \\(\\mathbf A\\) is sometimes called a two-mode or bipartite projection. The original network represented by \\(\\mathbf X\\) goes by many names—e.g, affiliation network, bipartite graph, two-mode network, etc.↩︎\nKnoke et al. (2021) [pp. 36] note three problems with using this sort of projection. First, it entails a loss of “identity information” on one of the two node sets—e.g., the cell entries in \\(\\mathbf A\\) don’t reveal the specific individuals shared across areas. Second, it results in very dense networks, which leads to biases in network measurements—e.g., an artificially high number of triangles (see Opsahl 2013). Third, it obscures the generative process behind tie formation.↩︎\nThe threshold level here is arbitrary, this is a blogpost. The “disparity filter” algorithm was chosen because it was the fastest, this is a blogpost.\n\nThe choice of algorithm and threshold level is rarely discussed in most publications of this sort, so I guess I’m sort of following tradition here.↩︎\nThe area of Ethnography (Anthropology)—which has about half the amount of individuals as Qualitative Methodology—has a connection to Urban Sociology, reflecting a long tradition of famous urban ethnographies.↩︎"
  },
  {
    "objectID": "posts/2022-02-07-theory-work/index.html",
    "href": "posts/2022-02-07-theory-work/index.html",
    "title": "Theory-Work",
    "section": "",
    "text": "Theoretical work should strive, among other things, “to improve the precision, clarity, and coherence of our ideas” (Martin 2014).\nThis involves cultivating good habits.\nFor example,\n\nOne important habit for theorizing is just the following: as one writes a period at the end of the sentence, to stop and ask oneself “am I sure this is true? Let me entertain the opposite. Do I have any reason to reject it other than my desire to go forward with my own argument?”\nMartin (2014, pp. 10–11)\n\nSometimes it involves avoiding common traps—e.g., the three “nuance traps” described by Kieran Healy:\n\nFirst is the ever more detailed, merely empirical description of the world. This is the nuance of the fine-grain. It is a rejection of theory masquerading as increased accuracy. Second is the ever more extensive expansion of some theoretical system in a way that effectively closes it off from rebuttal or disconfirmation by anything in the world. This is the nuance of the conceptual framework. It is an evasion of the demand that a theory be refutable. And third is the insinuation that a sensitivity to nuance is a manifestation of one’s distinctive (often metaphorically expressed and at times seemingly ineffable) ability to grasp and express the richness, texture, and flow of social reality itself. This is the nuance of the connoisseur. It is mostly a species of self-congratulatory symbolic violence\nHealy (2017, pp. 120–21)\n\nRelatedly, sociologists will sometimes stretch their concepts to fit new cases in such a way that brings about the careless redefinition of important terms. For example, Martin (2014) cites Niklas Luhmann saying that “All meaninglessness… has meaning again through its strangeness.” The statement is only true if we accept a new definition for the word meaning. And why should we?\n\nMost generally, whenever we find ourselves rushing to claim that “things outside of any set are themselves in the set” we may be changing our terminology in ways we do not understand. And if we make a habit of it, we’ll end up using meaningless statements.\nMartin (2014, p. 13)\n\nIn short, good theory-work should give us the tools to avoid common traps. It should enable us to tell apart good nuance (e.g. getting important details correct, avoiding oversimplifications) from bad nuance. It should enable us to tell apart good empirical tautologies (e.g., “people try to be good, and good is what ever people say is good”) from bad argumentative tautologies (i.e., assuming at one place what we are claiming to prove at another)."
  },
  {
    "objectID": "posts/2022-02-07-theory-work/index.html#deepities",
    "href": "posts/2022-02-07-theory-work/index.html#deepities",
    "title": "Theory-Work",
    "section": "Deepities",
    "text": "Deepities\nGood theory-work should also prevent us from saying deepities.\n\nA deepity is a proposition that seems both important and true —and profound— but that achieves this effect by being ambiguous. On one reading it is manifestly false, but it would be earth-shaking if it were true; on the other reading it is true but trivial. The unwary listener picks up the glimmer of truth from the second reading, and the devastating importance from the first reading, and thinks, Wow! That’s a deepity.\nDennett (2013, p. 56)\n\n\nDennett’s favorite example is the expression “love is just a word.”\n\nIn other words, good theory-work should constrain our thinking so that saying stupid things becomes much harder.\n\nConstructionist arguments are not deepities\nDeepities are found all over social science and the humanities, in the sense that ambiguous statements will get rewarded if they seem profound. A lot of constructionist arguments, for example, become deepities when they take the form “\\(X\\) is socially constructed, therefore \\(X\\) is not real”.\nBut most sociologists often take the opposite perspective: “\\(X\\) is real because it’s socially constructed”.1\nThe logic behind constructionist arguments is fairly straightforward.\n\nSocial constructionists about \\(X\\) tend to hold that:\n\n\\(X\\) need not have existed, or need not be at all as it is. \\(X\\), or \\(X\\) as it is at present, is not determined by the nature of things; it is not inevitable.\n\nVery often they go further, and urge that:\n\n\\(X\\) is quite bad as it is.\nWe would be much better off if \\(X\\) were done away with, or at least radically transformed.\n\nHacking (1999, p. 6)\n\nThis kind of argument is not trivial when \\(X\\) is taken for granted—i.e., when \\(X\\) appears to be inevitable. Take the notion of racecraft, which is used to describe how social hierarchy gets projected into “nature” in the form of “race.”\n\nConsider the statement “black Southerners were segregated because of their skin color”—a perfectly natural sentence to the ears of most Americans, who tend to overlook its weird causality.\nFields and Fields (2014, p. 17)\n\nHow can skin color cause segregation? How can skin color cause a stop-and-frisk incident?\nIn a way, this is the opposite of Dennet’s “deepity.” It is a statement that looks trivial and true at first glance. But, upon closer reflection, it has no well-formed meaning. And if it were true, it would have earth shattering consequences to our conception of causality. Which begs the question: why did it originally seem so unremarkable?\nAt their most ambitious, social constructionist arguments are an indictment of folk social theory (or “the culture”). They remind us that many arbitrary things come to be seen as “natural” or “inevitable” in the course of everyday life."
  },
  {
    "objectID": "posts/2022-02-07-theory-work/index.html#footnotes",
    "href": "posts/2022-02-07-theory-work/index.html#footnotes",
    "title": "Theory-Work",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is what drove Durkheim to speak of social facts and it’s what drives contemporary sociologists to speak of institutions.↩︎"
  },
  {
    "objectID": "posts/2024-01-04-fetishism-and-the-variables-paradigm/index.html",
    "href": "posts/2024-01-04-fetishism-and-the-variables-paradigm/index.html",
    "title": "Fetishism and The “Variables Paradigm”",
    "section": "",
    "text": "Karl Marx’s criticism of commodity fetishism is very similar to Andrew Abbott’s criticism of the so-called “variables-based approach” in sociology (i.e., most standard quantitative research).\nThe former denounces the idea of “value” as an alleged property of commodities (and not people doing things) to be a capitalist illusion. The latter denounces the idea of “causality” as an alleged property of variables (and not people doing things) to be a positivist illusion.\nBoth criticism are usually blown out of proportions, mostly because it is rare that someone out there is really under the trappings of such “illusions” in the first place.\nBefore getting into detail, let me note the following:"
  },
  {
    "objectID": "posts/2024-01-04-fetishism-and-the-variables-paradigm/index.html#commodity-fetishism",
    "href": "posts/2024-01-04-fetishism-and-the-variables-paradigm/index.html#commodity-fetishism",
    "title": "Fetishism and The “Variables Paradigm”",
    "section": "Commodity Fetishism",
    "text": "Commodity Fetishism\n\nA commodity appears at first sight an extremely obvious, trivial thing. But its analysis brings out that it is a very strange thing, abounding in metaphysical subtleties and theological niceties.\n— Karl Marx, Capital\n\nThe commodity fetishism argument is an invitation to look beyond the surface of appearances, an illusion according to which “the definite social relation between men themselves” assumes “the fantastic form of a relation between things.”\nThe accusation is that normal people see value as an intrinsic property of commodities and fail to see it for what it is: a relational property.\nThis is the best two-paragraph explanation I’ve seen of what all this means.\n\nThere are two ways of ascribing properties to objects. Both have the same surface grammatical form: \\(A\\) is \\(F\\). The book is red, the man is tall, the woman is rich. They differ, however, at a deeper level. The height of a person is a quality that inheres in him quite independently of social context. Wealth, on the other hand, can only be predicated of a person who is inserted in a web of social relations. It makes sense to say that Robinson Crusoe on his island was tall, not that he was rich, even if perhaps he brought some gold coins along with him. To be rich means that other people are willing to exchange their goods or labor for your money. Being rich, unlike being tall, is a relational predicate.\n[…]\nCommodity fetishism is the belief that goods possess value just as they have weight, as an inherent property. To the unmystified mind, it is clear that a commodity has exchange value only because it stands in certain relations to human labor and human needs. In the bewitched world of commodity fetishism, however, goods appear to exchange at a certain rate because of their inherent values. Such, at any rate, was Marx’s argument. It is somewhat unconvincing, because it is hard to believe that anyone ever committed this particular fallacy.\nElster (1986, pp. 56–57, emphasis added)\n\nI agree. It’s hard to believe that anyone has ever thought about the value of commodities in this way. But being more sophisticated than others makes us feel good and motivated reasoning is one helluva drug (Kunda 1990).\nNote. Other accusations of economic fetishism are more on point. For example, the idea that “money” has an intrinsic value (especially in the form of gold or silver) has been an amply documented illusion throughout history. A similar dynamic happens with “capital” when it’s considered as raw materials or instruments of labor; the “stuff” we identify as capital only becomes capital when it’s embedded in a process of accumulation (M-C-M′).\nThus, arguments about fetishism may hold true in certain domains (e.g., money, capital), but we should doubt about the truthfulness of the more narrow commodity fetishism argument."
  },
  {
    "objectID": "posts/2024-01-04-fetishism-and-the-variables-paradigm/index.html#the-variables-paradigm",
    "href": "posts/2024-01-04-fetishism-and-the-variables-paradigm/index.html#the-variables-paradigm",
    "title": "Fetishism and The “Variables Paradigm”",
    "section": "The “Variables Paradigm”",
    "text": "The “Variables Paradigm”\nSome time ago Andrew Abbott (2001) mounted an attack on traditional quantitative researchers by accusing them of foolishly believing that “variables do things, not social actors.”\nHere are the charges:\n\nQuantitative researchers simplify too much.\nThey forget the importance of narrative.\nThey are plain silly.\nThey don’t see individuals as motivated actors, but merely as the locale for the variables “doing their thing” (Abbott 2001, p. 132). For example, they actually believe that some abstraction called education acts on another abstraction called occupation.\n\nOur normal methods parse social reality into fixed entities with variable qualities. They attribute causality to the variables—hypostatized social characteristics—rather than to agents; variables do things, not social actors.\nAbbott (2001, p. 183)\n\nThey ignore that social activity is located space and time, unlike the always rich and sophisticated Chicago School.\n\nFor the idea of a variable is the idea of a scale that has the same causal meaning whatever its context: the idea, for example, that “education” can have “an effect” on “occupation’ irrespective of the other qualities of an individual, whether those qualities be other past experiences, other personal characteristics, or friends, acquaintances, and connections. Within variable-based thinking, one allows for a few”interactions” to modify this single causal meaning contextually, but the fundamental image of variables’ independence is enshrined in the phrase “net of other variables” and in the aim to discover this net effect, whether through experimental or statistical manipulation. The Chicago view was that the concept of net effect was social scientific nonsense. Nothing that ever occurs in the social world occurs “net of other variables.” All social facts are located in contexts. So why bother to pretend that they are not?\nAbbott (1997, p. 1152)\n\nExcept it’s not nonsense!\n\nThe problem with this discussion is that the target of criticism—the so-called “variables paradigm”—does not exist. It has never existed. It’s hard to believe that someone out there is really that dumb.\nIn a particularly egregious example of bad faith, Andrew Abbott reads a couple of articles from the American Journal of Sociology and finds that “narrative sentences usually have variables as subjects” (Abbott 2001, p. 140).1 Now imagine overhearing a conversation in which someone mentions the sunset and then concluding they’re being foolish fool for believing the sun revolves around the earth. It is deeply obnoxious. We must be able to distinguish between convenient “figures of speech” and actual “beliefs.”\nNote. After writing the above I came across Mustafa Emirbayer’s (1997) “Manifesto for a Relational Sociology.”\nTL;DR it’s not good, it’s mostly nonsense.\nAmong other things, he argues that it’s futile for quantitative researchers to adjust for confounders because all such attempts “ignore the ontological embeddedness or locatedness of entities within actual situational contexts” (Emirbayer 1997, p. 289).\nYikes!"
  },
  {
    "objectID": "posts/2024-01-04-fetishism-and-the-variables-paradigm/index.html#data-and-phenomena",
    "href": "posts/2024-01-04-fetishism-and-the-variables-paradigm/index.html#data-and-phenomena",
    "title": "Fetishism and The “Variables Paradigm”",
    "section": "Data and Phenomena",
    "text": "Data and Phenomena\nAs mentioned in a previous blogpost, I find it useful to distinguish between data and phenomena:\n\nPhenomena: recurrent features of the world.\nData: “public records produced by measurement and experiment that serve as evidence for the existence or features of phenomena” (Woodward 2011, p. 166)\n\nCriticism of the so-called “variables paradigm” may be valid in the rare circumstances in which researchers forget this distinction; or in cases in which people conflate data mining with the scientific modeling of real-world phenomena.\nSelf-proclaimed critical thinkers might complain and insist that quantitative research is riddled with variable fetishism; that GLMs and DAGs merely depict relationships between variables, not between real-world phenomena; and that convenient figures of speech reveal some kind of weird metaphysics.\nBut, I assure you, no one out there really thinks that variables “cause” other variables in a social vacuum."
  },
  {
    "objectID": "posts/2024-01-04-fetishism-and-the-variables-paradigm/index.html#on-trained-incapacities",
    "href": "posts/2024-01-04-fetishism-and-the-variables-paradigm/index.html#on-trained-incapacities",
    "title": "Fetishism and The “Variables Paradigm”",
    "section": "On Trained Incapacities",
    "text": "On Trained Incapacities\nNote. I added this section after receiving some useful feedback from friends.\nHere’s a more charitable view of Andrew Abbott’s full-blown attack on linear regression.\n\nTraditional linear regression—particularly when used to model cross-sectional datasets—is ignorant of temporal context in ways that are counterproductive. The significance of some measured “variables” is severely distorted when they’re ripped from their temporal context, making some research questions impossible to answer.\nIn particular, the order in which things happen determines the outcomes that most historical researchers are interested in explaining.\nA similar thing happens when we want to answer questions for which spatial context is important. This is why there’s a whole subfield of statistics dedicated to modeling “spatial data.”\nPaul Pierson has a great culinary analogy for how time “works” in linear regression for cross-sectional data:\n\nImagine that your friend invites you to the trendiest new restaurant in town, charmingly named “The Modern Social Scientist.” As an added bonus, he informs you that he knows the chef well, and that you will have a chance to tour the kitchen. When you arrive, the chef explains that the kitchen is divided into two parts. On the left, she has all the ingredients which to your puzzlement she refers to as “variables”). These ingredients, she insists, are the freshest available and carefully selected. On the right is an extraordinary profusion of measuring devices. You express astonishment at their complexity and detailed ornamentation, and the chef explains that each requires years to learn how to operate properly.\nThe chef proceeds to elaborate her culinary approach: good cooking, she says, amounts to having the perfect ingredients, perfectly measured. Traditional cooks have stressed how important the cooking process itself is, including the sequence, pace, and specific manner in which the ingredients are to be combined. Not so, says the proprietor of The Modern Social Scientist. As long as you have the correct ingredients and they are properly measured, she insists, how, in what order, and for how long they are combined makes no difference.\nPierson (2011, p. 1)\n\nBut quantitative research does not have to be decontextualized in this way. There are many ways of adapting quantitative research to answer some of the criticisms levied by scholars like Pierson and Abbott (e.g., Wawro and Katznelson 2022).\nActually, thinking about temporal assumptions in linear regression can be very helpful—e.g., did you know that fixed-effect models produce wrong estimates when the “causal lags in the real world do not match the lags found in panel data” (Vaisey and Miles 2017)?\nHowever, there is only one correct way of answering to someone calling you an unsophisticated fool with weird metaphysics: silence.\nSpecializing in quantitative methods produces a sort of “trained incapacity,” limiting the scope of questions that researchers can answer. People that specialize in qualitative methods are usually not experts in quantitative research; people that specialize in social network analysis are usually not experts in archival research; and so on.\nNote. Sociology is actually better than most other disciplines at producing researchers that defy these expectations (e.g., Mario Small, Emily Erikson, Peter Bearman).\nHowever, there’s no reason to think that becoming competent in one form of social research will make you less competent in another.\n\nThis last point is worth repeating. I worry that students will stumble upon criticism of the “variables paradigm” and conclude that learning about linear regression will actually make them worse sociologists. I had a similar reaction about a decade ago.\nI think this quote by Arthur Stinchcombe sums it best:\n\nA “trained incapacity” is produced by a sufficient level of specialization so that some kinds of good work cannot be produced by some of the competent practitioners in the discipline. If it takes so much time to learn statistics up to a modern standard that quantitative researchers do not have time to learn any history, or if the incentive system among quantitative people discourages dillentantism, or if quantitative workers have a positive ideology against becoming learned in “soft” subjects, or if the social circles that know about hierarchical models do not know about recent good books, then quantitative people will not be able to see things that one can find out about only historically. What one has to look for is not competencies, for there is no general reason that one competence should interfere with another (in fact in general disparate competencies are positively correlated, though generally the correlation is small). Instead one has to look for tendencies for competence in one thing to decrease the ability to develop competence at another thing.\nStinchcombe (1986, p. 25, emphasis added)"
  },
  {
    "objectID": "posts/2024-01-04-fetishism-and-the-variables-paradigm/index.html#footnotes",
    "href": "posts/2024-01-04-fetishism-and-the-variables-paradigm/index.html#footnotes",
    "title": "Fetishism and The “Variables Paradigm”",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“My procedure is simple. I find all the narrative sentences, the sentences whose predicates are activities or parts of the social process. I then consider who are the subjects of these sentences, what kinds of activities are involved, and how the predicates are related to causality” (Abbott 2001, p. 130).↩︎"
  },
  {
    "objectID": "posts/2024-03-10-path-dependence/index.html",
    "href": "posts/2024-03-10-path-dependence/index.html",
    "title": "Path Dependence",
    "section": "",
    "text": "A path-dependent explanation can usually be decomposed into two causal components: (1) an original cause and (2) a maintenance structure. More contemporary formulations refer to this same distinction in terms of “critical junctures” and “mechanisms of reproduction.”\n\nThe first is the particular circumstances which caused a tradition to be started. The second is the general process by which social patterns reproduce themselves.\nStinchcombe (1968, pp. 102–3)\nSome initial event or process generates a particular outcome, which is then reproduced through time even though the original generating event or process does not recur.\nPierson (2011, p. 45)\n\nAccording to Pierson (2011), path-dependent (or self-reinforcing) processes share at least four defining features:\n\n\nMultiple equilibria. Under a set of initial conditions conducive to positive feedback, a range of outcomes is generally possible.\nContingency. Relatively small events, if occurring at the right moment, can have large and enduring consequences.\nA critical role for timing and sequencing. In these path-dependent processes, when an event occurs may be crucial. Because early parts of a sequence matter much more than later parts, an event that happens “too late” may have no effect, although it might have been of great consequence if the timing had been different.\nInertia. Once such a process has been established, positive feedback will generally lead to a single equilibrium. This equilibrium will in turn be resistant to change.\n\nPierson (2011, p. 44)\n\nThe preferred metaphor is that of a Polya urn process.\nHere’s some R code:\n\n\nPackages\nlibrary(tidyverse)\ntheme_set(theme_light(base_family = \"Avenir Next Condensed\"))\n\n\n\n\nCode\nurn_process &lt;- function(N) {\n  out &lt;- purrr::accumulate(.init = 0:1, .x = 2:N, .f = \\(u, ...) {\n    ## draw random sample\n    draw &lt;- sample(u, size = 1)   \n    ## add draw to urn\n    c(u, draw)                    \n  })\n  ## calculate prop at each time step\n  purrr::map_dbl(out, mean)\n}\n\n\n\nImagine a very large urn containing two balls, one black, one red. You remove one ball, and then return it to the urn along with an additional ball of the same color. You repeat this process until the urn fills up. What can we say about the eventual distribution of colored balls in the urn? Or about a series of trials in which we fill the urn and then start over again one hundred times?\nPierson (2011, p. 17)\n\nAnd here is how 15 independent processes could look like:\n\n\nCode\nR &lt;- 15\n\nM &lt;- replicate(R, urn_process(500), simplify = FALSE)\nnames(M) &lt;- 1:R\n\nd &lt;- as_tibble(M) |&gt; \n  rowid_to_column(var = \".id\") |&gt; \n  pivot_longer(!.id, names_to = \"path\", values_to = \"prop\")\n\nd |&gt; \n  ggplot(aes(.id, prop, group = path, color = path)) + \n  geom_line(linewidth = 1/3, show.legend = FALSE) + \n  scale_y_continuous(labels = scales::percent) + \n  coord_cartesian(ylim = c(0, 1))  +\n  scale_color_grey() +\n  labs(\n    x = \"Trial Number\", \n    y = \"Percent of red balls in the urn\", \n    title = str_glue(\"{R} Independent Polya Urn Processes\")\n  )\n\n\n\n\n\n\n\n\n\nThe first few trials correspond to a “critical juncture” and the eventual equilibrium is explained by the sampling procedure in place.\n\n\n\n\nReferences\n\nPierson, Paul. 2011. Politics in Time. Princeton University Press.\n\n\nStinchcombe, Arthur L. 1968. Constructing Social Theories. University of Chicago Press."
  },
  {
    "objectID": "posts/2024-01-11-better-theories/index.html",
    "href": "posts/2024-01-11-better-theories/index.html",
    "title": "“Better Theories”",
    "section": "",
    "text": "The new editors of Theory & Society have a strange new statement of goals with some minor platitudes about what makes for a good theory—e.g., “general, abstract, and amenable to empirical investigation.”\nBut most of the statement is quite superficial in content and angry in tone.\nSo, what actually makes for a good “scientific theory”?\nI’m just going to copy-paste this list of good principles:\n\nFORMAL VIRTUES\n\nTestability\nThe theory should express commitments about the world that in principle can be confirmed or falsified on the basis of empirical evidence. \nInternal Coherence\nThe theory should be coherent and should not contain contradictions.\n\nPRAGMATIC VIRTUES\n\nFertility\nThe theory should suggest new and exciting avenues of research. It should generate new research questions faster than it can answer them. \nConservatism\nThe theory should retain crucial bits of what came before and not break too quickly or too dramatically with tradition without a compelling reason for doing so.\n\nAESTHETIC VIRTUES\n\nSimplicity\nThe theory should posit only those entities, properties, causal relations, etc. that are necessary to account for the phenomenon.\nElegance\nThe theory should be compact and graceful.\n\nEMPIRICAL VIRTUES\n\nEmpirical Adequacy\nThe theory should accommodate or fail to conflict with well-established phenomena.\nPrediction\nThe theory should make accurate predictions and retrodictions, particularly concerning phenomena that would be surprising were the theory to be false.\nExplanation\nThe theory should explain the phenomena in its domain either by showing how they follow from general laws of nature or by showing how they are produced, given rise to, or maintained by mechanisms.\nExternal Coherence\nThe theory should be supported by (or at least consistent with) other well-accepted non-rival theories.\nGenerality\nThe theory should apply to more phenomena than its rivals.\nUnification\nThe theory should unify diverse phenomena by showing them to be instances of a common pattern.\n\nFrom: Craver and Darden (2013, pp. 83–84)\n\nNote. Craver and Darden (2013) advocate for a mechanism-based view of scientific explanation which nowadays has superseded the “covering law” approach to scientific explanation (i.e., 20th century positivism). Mechanism-based explanations have been heavily endorsed by “analytical sociology” (e.g., Hedström and Swedberg 1996; Hedstrom 2005); unfortunately, most sociologists see this literature as being exclusively commited to rational-choice theory and agent-based modeling. Read the book!\n\n\n\n\nReferences\n\nCraver, Carl F., and Lindley Darden. 2013. In Search of Mechanisms: Discoveries Across the Life Sciences. University of Chicago Press.\n\n\nHedstrom, Peter. 2005. Dissecting the Social: On the Principles of Analytical Sociology. Cambridge: Cambridge University Press.\n\n\nHedström, Peter, and Richard Swedberg. 1996. “Social Mechanisms.” Acta Sociologica 39(3): 281–308."
  },
  {
    "objectID": "posts/2024-01-25-levels/index.html",
    "href": "posts/2024-01-25-levels/index.html",
    "title": "Levels",
    "section": "",
    "text": "The metaphor of levels is used to describe the world all the time.\nThese metaphors are too basic to how we organize the world. Scientists use them all the time. Rejecting these metaphors just because some obscure sociologist made poor use of them is laughable. So, the word “level” is inevitable, but it is also vague and invites confusion.\nProblems\nMost problems with the levels metaphor stem from being committed to the following:\nThese commitments seem innocuous and maybe obvious at times. For example, Andrew Abbott relies on this metaphor to offer an explanation as to why interdisciplinarity seems more straightforward in the natural sciences than in the social sciences.\nSounds insightful, right? Maybe, a little bit, but it’s also very misleading.\nMost disciplines cast a wide net. The relationship between scientific disciplines and “ontological levels” is many-to-many. For example, evolutionary biology encompasses genes, organisms, populations, and environments. Take a look at virtually any scientific discipline and you’ll find the same.\nThe rest of this entry focuses on more legitimate ways of using the levels metaphors in different contexts. I will focus on two kinds of levels that I find useful: (1) levels of description, in which we look at the same phenomena from different perspectives; and (2) levels of organization, in which we look at relationships between “wholes” and their relevant “component parts.”\nNote. I will ignore three other commonly used kinds of levels: scale, processing, and disciplinary topic.2"
  },
  {
    "objectID": "posts/2024-01-25-levels/index.html#levels-of-description",
    "href": "posts/2024-01-25-levels/index.html#levels-of-description",
    "title": "Levels",
    "section": "Levels of Description",
    "text": "Levels of Description\nLevels of description indicate different ways of looking at the same problem or phenomena. Here, the differences between levels are a matter of perspective.\nHere, talk of different “levels” usually means we are looking at the same phenomena from different perspectives. As such, these levels don’t raise issues of “emergence” of “higher” levels (or at least they shouldn’t).\nThese levels are legitimate insofar they make important (non-redundant) contributions to understanding whatever it is we want to understand.\nI will focus on two well-established levels of description:\n\nDavid Marr’s hierarchy for the analysis of information-processing systems, in which levels refer to realization (or implementation).\nThe distinction between social structure and culture, in which levels stem from the classic philosophical distinction between form and content.\n\n\nRealization\nMarr’s famous hierarchy of analysis consists of three levels, summarized in Table 1. It is supposed to help us understand complex information-processing “machines.”\n\nThe computational level.\nWhat is being computed and why?\nIn this framework, the “what” is usually a mathematical calculation (e.g., optimization, prediction); and the “why” is usually determined by looking at the environment in which the information-processing system is embedded.\nThe representation and algorithm level.\nInformation-processing systems must represent their inputs and outputs in accordance to some formal scheme.3 This is important because “any particular representation makes certain information explicit at the expense of information that is pushed into the background and may be quite hard to recover” (Marr [1982] 2023, p. 60).\nResults at this level can be established without regard to the physical entities actually instantiating the algorithm—e.g., is the computation tractable (e.g., Kleinberg 2000; Van Rooij 2008)?\nThe physical implementation level.\nAn account of physical computation is outside the scope of this entry (see Piccinini 2015).\n\nThese are levels of realization. A computation is implemented by an algorithm, which in turn is implemented by some kind of physical system. The analysis at each level can be partially decoupled from the others because of the general notion of multiple realizability—i.e., the same computation can be realized by a variety of algorithms; and the same algorithm can be realized by a variety of physical entities.\n\nThe choice of an algorithm is influenced for example, by what it has to do and by the hardware in which it must run. But there is a wide choice available at each level, and the explication of each level involves issues that are rather independent of the other two.\nMarr ([1982] 2023, p. 63)\n\n\n\n\n\n\n\n\n\n\n\n\nComputational theory\nRepresentation and algorithm\nHardware implementation\n\n\n\n\nWhat is the goal of the computation, why is it appropriate, and what is the logic of the strategy by which it can be carried out?\nHow can this computational theory be implemented? In particular, what is the representation for the input and output, and what is the algorithm for the transformation?\nHow can the representation and algorithm be realized physically?\n\n\n\n\n\nTable 1: The three levels at which any machine carrying out an information-processing task must be understood.\n\n\n\n\nSource: David Marr ([1982] 2023, p. 64)\n\nMore importantly, the three levels also constrain each other.\n\nWorking from the top down, knowing the structure of the environment and the information it makes available to the organism limits the types of information-processing algorithms that can utilize that information. Likewise, having identified a mode of organization and the conditions under which it will generate a form of behavior can guide the search for the components that implement the design. Constraints also arise from the bottom up. Knowing features of the implementation can put constraints on the search for algorithms. Some algorithms might not be implementable, given the components available, and alternatives must be sought. Likewise, knowing the algorithm that seems to be functioning can guide investigations into the environment and reveal different features of its structure that are relevant to the organism.\nBechtel and Shagrir (2015, p. 321)\n\n\n\nStructure and Culture\nMany sociologists—following in the footsteps of Georg Simmel—like to distinguish between structure and culture (e.g., Rawlings et al. 2023). These perfectly overlapping “levels” stand in opposition to the level of individuals (a distinction that roughly corresponds to a parts-to-whole relationship). But the difference between structure and culture roughly corresponds to a distinction between form and content.\nPerspectives that privilege structure have produced many important ideas such as the strength of weak ties, structural holes, and vacancy chains. Perspectives that privilege culture emphasize ideas such as roles, rules, categories, schemas, and legitimacy.\nThe idea is that the analysis of each level can be partially decoupled from the other, even though each level is just a different way of representing the same phenomena. They are different sides of the same coin. Each level makes up the blind spot of the other.\nSome novel forms of network analysis attempt to incorporate both levels simultaneously (Lizardo 2023a; e.g., Lizardo 2023b)."
  },
  {
    "objectID": "posts/2024-01-25-levels/index.html#levels-of-organization",
    "href": "posts/2024-01-25-levels/index.html#levels-of-organization",
    "title": "Levels",
    "section": "Levels of Organization",
    "text": "Levels of Organization\nThe levels metaphor is most useful when it distinguishes between wholes and their component parts.4\nThe main idea behind mechanistic explanations is to figure out how the organized activities of parts produce some property or behavior of a larger whole (Machamer et al. 2000; Craver 2001; Craver and Bechtel 2007; Craver and Darden 2013). Here, the levels metaphor relates some activity or feature of a whole to the properties and organized activities of relevant component parts. This is what it means to describe a mechanism: the component parts work together to accomplish something that they cannot do on their own. Mechanisms— by definition—are more than the sum of their parts.\nThis provides for a view of levels that is very different from the notion of ontological “layers”; “Levels of mechanisms cannot be read off a menu of levels in advance” (Craver 2014, p. 18).\nFor example, a researcher might be interested in explaining how human groups accomplish complex tasks—e.g., spatial learning and navigation (Hutchins 1995; Vaughan 2021). Or they might be interested in figuring out how status hierarchies enhance the performance of task-oriented groups (Willer 2009). Or they might simply be an organizational sociologist.\nMechanism levels are not spooky\nA mechanism approach to levels also guards against “spooky emergentism” or ontologically unmoored macro abstractions—i.e., what happens when sociologists commit to the existence of higher-level phenomena that have no explanation in terms of the organized activities of their component parts. Nothing “emerges” from levels of mechanisms except in the trivial sense that organized parts do things that they cannot do in isolation.\nImportantly, mechanistic explanations are not committed to methodological individualism—i.e., the view according to which good explanations must always point towards individuals and their choices.\n\nIt is not part of our view that all explanations must bottom out in some privileged set of fundamental entities and activities…\nNor is it part of our view that all mechanistic explanations must go down in a hierarchy of mechanisms to be explanatory. In order to explain why a given strand of DNA is on a particular island in the Galapagos, one would do better to investigate the migratory patterns of birds and the selective effects of a recent drought, rather than the molecular components of its DNA. One must, in such cases, ascend from the part to the whole to understand the relevant mechanisms. If one wants to understand why a cyclist’s cellular glucose metabolism suddenly accelerated, the right answer might well be that the cyclist has started the climb.\nCraver and Darden (2013, p. 25)\n\nThis is compatible with the view of multi-level explanation in sociology.\n\n…explanations at one level may refer to causal processes at another. If a soldier shoots another soldier, it may have nothing to do with their individual properties or relations: it may simply be that two armies are at war, and thus it might be pointless to study causality as arising from the personalities involved. On the other hand, causal explanation of a societal process—say, a decline in a birth rate—may properly invoke little more than changes in the individuals involved (e.g., a rise in education produces women who prefer to have fewer children). The point here is that the content of an explanation must be worked out empirically, considering possible connections at multiple levels of analysis, sorting out which connections are present and substantial, and which are not.\nJepperson and Meyer (2021, p. 199)"
  },
  {
    "objectID": "posts/2024-01-25-levels/index.html#footnotes",
    "href": "posts/2024-01-25-levels/index.html#footnotes",
    "title": "Levels",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSome social scientists find this “ontological stratification” of the world very attractive because they want to protect themselves against incursions from nearby disciplines—e.g., psychologists protecting themselves against the neuroscientists, sociologists protecting themselves against the psychologists economists. They further complicate matters by developing some goofy view about the autonomy or irreducibility of their preferred level.↩︎\nI will also ignore the related (and important) notion of units of analysis—i.e., levels are individuated (made distinct) by units of measurement (e.g., individuals, organizations, groups, populations).↩︎\n“To say that something is a formal scheme means only that it is a set of symbols with rules for putting them together—no more and no less” (Marr [1982] 2023, p. 60).↩︎\n“Levels of parts and wholes must also be correlated with size differences because parts can be no larger than the wholes they compose. But the size differences between levels of parts and wholes are an accidental consequence of the part-whole relationship itself, not part of defining what it is for things to be at different part-whole levels” (Craver 2014, p. 12).↩︎"
  },
  {
    "objectID": "posts/2023-12-27-what-is-computational-social-science/index.html",
    "href": "posts/2023-12-27-what-is-computational-social-science/index.html",
    "title": "What is Computational Social Science?",
    "section": "",
    "text": "I don’t think Computational Social Science (CSS) will ever become an autonomous field.\nCSS is more akin to a “trading zone” in which different disciplinary cultures manage to exchange ideas, techniques, and metaphors.1\nAs such, there’s no point in creating exclusionary boundaries around CSS. That would be bad for trade.2\nCSS is different from previous interdisciplinary efforts in that it is (1) mainly driven by advances in digital technology and (2) it is associated with a new professional group."
  },
  {
    "objectID": "posts/2023-12-27-what-is-computational-social-science/index.html#footnotes",
    "href": "posts/2023-12-27-what-is-computational-social-science/index.html#footnotes",
    "title": "What is Computational Social Science?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe idea of “trading zones” in science goes back to historian Peter Galison.↩︎\nMatt Salganik has made this point before, although for slightly different reasons; he jokingly argues that we should be happy with defining CSS simply as “anything that’s cool.” See An Introduction to Computational Social Science.↩︎\nSpector et al. (2022, p. 7) define data science as “the study of extracting value from data—value in the form of insights or conclusions.” It’s impossible to create exclusionary boundaries around such an open definition. But that should not matter as long as companies continue to recruit and students continue to enroll.↩︎\nSee Kline (2015) for cybernetics. See Thagard (2023) for cognitive science.↩︎\nTwo examples come to mind. The idea that we should pay more attention to the algorithmic component of small-world networks (Kleinberg 2000) and the idea that we should consider word embeddings as providing a template for cultural learning (Arseniev-Koehler and Foster 2022; cf. Landauer and Dumais 1997).↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Path Dependence\n\n\n\n\n\n\nTheory\n\n\nSociology\n\n\nComputation\n\n\n\nPolya Urns and the “frozen accidents” of history \n\n\n\n\n\nMar 10, 2024\n\n\nandrés castro araújo\n\n\n\n\n\n\n\n\n\n\n\n\nLevels\n\n\n\n\n\n\nTheory\n\n\nSociology\n\n\n\nI have some thoughts. \n\n\n\n\n\nJan 25, 2024\n\n\nandrés castro araújo\n\n\n\n\n\n\n\n\n\n\n\n\n“Better Theories”\n\n\n\n\n\n\nTheory\n\n\nSociology\n\n\n\nFormal, pragmatic, aesthetic, and empirical virtues \n\n\n\n\n\nJan 11, 2024\n\n\nandrés castro araújo\n\n\n\n\n\n\n\n\n\n\n\n\nFetishism and The “Variables Paradigm”\n\n\n\n\n\n\nSociology\n\n\nTheory\n\n\n\nAre all regression coefficients riddled with metaphysical subtleties and theological niceties? \n\n\n\n\n\nJan 4, 2024\n\n\nandrés castro araújo\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Sociology in the USA?\n\n\n\n\n\n\nSociology\n\n\nNetworks\n\n\n\nA visualization. \n\n\n\n\n\nJan 2, 2024\n\n\nandrés castro araújo\n\n\n\n\n\n\n\n\n\n\n\n\nData\n\n\n\n\n\n\nPhilosophy\n\n\nStatistics\n\n\n\nWhat is data, if not experience persevering? \n\n\n\n\n\nDec 31, 2023\n\n\nandrés castro araújo\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Computational Social Science?\n\n\n\n\n\n\nSocial Science\n\n\nComputation\n\n\n\nIt is a trading zone, driven by improvements in digital technology and associated with a new professional group. \n\n\n\n\n\nDec 27, 2023\n\n\nandrés castro araújo\n\n\n\n\n\n\n\n\n\n\n\n\nDistributions in R\n\n\n\n\n\n\nR\n\n\nProbability\n\n\n\nA short tutorial. \n\n\n\n\n\nMay 12, 2022\n\n\nandrés castro araújo\n\n\n\n\n\n\n\n\n\n\n\n\nTheory-Work\n\n\n\n\n\n\nTheory\n\n\nSociology\n\n\n\nDeepities, social constructionism, and sociological theory. \n\n\n\n\n\nFeb 7, 2022\n\n\nandrés castro araújo\n\n\n\n\n\n\n\n\n\n\n\n\nRegression to the Mean\n\n\n\n\n\n\nStatistics\n\n\nCausality\n\n\n\nA visual explanation. \n\n\n\n\n\nJan 1, 2022\n\n\nandrés castro araújo\n\n\n\n\n\n\n\n\n\n\n\n\nOptimization in R\n\n\n\n\n\n\nR\n\n\n\nA few different ways to do optimization. \n\n\n\n\n\nJan 24, 2021\n\n\nandrés castro araújo\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-12-31-data/index.html",
    "href": "posts/2023-12-31-data/index.html",
    "title": "Data",
    "section": "",
    "text": "Data, plural for the Latin word datum.\nI find it useful to distinguish between data and phenomena.\n\nPhenomena: recurrent features of the world.\nData: “public records produced by measurement and experiment that serve as evidence for the existence or features of phenomena” (Woodward 2011, p. 166)\nEven in well-designed experiments, the data will reflect the influence of many other causal factors that have nothing to do with the phenomena of interest.\n\nBut there’s more.\nWhen we think about “data” we generally think about standardized data—i.e., symbols that are arranged in a way that’s convenient for computer processing. We think about symbols that are stable, manipulable, and transportable—or what Bruno Latour (1987) calls “immutable and combinable mobiles.”\nNote. Accepting a distinction between data and phenomena means that we can have different explanations for the phenomena (scientific models) and for the observed data (statistical models). The most obvious examples of this distinction are statistical models whose purpose is to account for some form of measurement error or to impute missing data. All variables are measured with error. The distinction becomes less clear when we start thinking about “bespoke statistical models” (McElreath 2020, chap. 16).\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nLatour, Bruno. 1987. Science in Action: How to Follow Scientists and Engineers Through Society. Harvard University Press.\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in r and Stan. CRC press.\n\n\nWoodward, James F. 2011. “Data and Phenomena: A Restatement and Defense.” Synthese 182: 165179."
  },
  {
    "objectID": "posts/2022-05-12-distributions-in-r/index.html",
    "href": "posts/2022-05-12-distributions-in-r/index.html",
    "title": "Distributions in R",
    "section": "",
    "text": "Set up\nlibrary(tidyverse)\ntheme_set(\n  theme_light(base_family = \"Optima\") + \n  theme(strip.background = element_rect(fill = \"#595959\"))\n)\nR has four built-in forms of working with probability distributions.\nSee ?distributions for a list of common distributions contained in R.\nFor example, to work with a normal distribution we have the following functions:\nA common source of confusion comes from the difference between continuous random variables (e.g. a normal distribution) and discrete random variables (e.g. a binomial distribution).\nThis will all make sense."
  },
  {
    "objectID": "posts/2022-05-12-distributions-in-r/index.html#discrete-distributions",
    "href": "posts/2022-05-12-distributions-in-r/index.html#discrete-distributions",
    "title": "Distributions in R",
    "section": "discrete distributions",
    "text": "discrete distributions\nIn this section we’ll use two distributions as examples.\nThe probability distribution of a binomial random variable comes from adding coin flips (also known as Bernoulli distributions). The Bernoulli distribution has two possible outcomes \\(x = \\{0, 1\\}\\) and one parameter \\(p\\) (which confusingly is also a probability).\nFor example, let’s suppose a coin is loaded and so \\(p = 0.75\\).\nThe probability mass function (PMF) of this Bernoulli distribution is as follows:\n\\[\nf(x) = \\Pr(X = x) = \\begin{cases}\n    0.75 &\\text{if} \\ x = 1 \\\\\\\\\n    0.25 &\\text{if} \\ x = 0 \\\\\\\\\n    0 &\\text{if} \\ x = \\text{anything else}\n\\end{cases}\n\\]\nThe cumulative distribution function (CDF) is as follows:\n\\[\nF(x) = \\Pr(X \\leq x) = \\begin{cases}\n    0 &\\text{if} \\ x &lt; 1 \\\\\n    0.25 &\\text{if} \\ x = 0 \\\\\n    1  &\\text{if} \\ x = 1 \\\\\n    1 &\\text{if} \\ x &gt; 1\n\\end{cases}\n\\]\n\nNote the change in notation from \\(f\\) to \\(F\\).\n\nAny CDF returns the probability that an outcome is less than or equal to \\(x\\). In other words, you’re simply adding up the probability masses for each possible outcome until you reach \\(x\\).\nThe binomial distribution\nAs mentioned earlier, the binomial distribution comes from adding \\(n\\) coin flips. For example, if you throw 3 coins then we have four possible outcomes \\(x = \\{0, 1, 2, 3\\}\\) and two parameters: \\(p\\) and \\(n = 3\\).\nThe probability (mass) function of this binomial distribution is then this:\n\\[\nf(x) = \\Pr(X = x) = \\begin{cases}\n    1 \\ (1 - p)^3 &\\text{if} \\ x = 0 \\\\\n    3 \\ p(1 - p)^2 &\\text{if} \\ x = 1 \\\\\n    3 \\ p^2(1-p) &\\text{if} \\ x = 2 \\\\\n    1\\ p^3 &\\text{if} \\ x = 3 \\\\\n    0 &\\text{if} \\ x = \\text{anything else}\n\\end{cases}\n\\]\nThe 1s and 3s come from counting the number of ways in which \\(x\\) can equal one of these numbers. This is not different from “the garden of forking data” stuff in McElreath (2020, pp. 20–27)\nBut this is not how you’ll see binomial distributions written out in the wild. We need new notation in order to write any binomial distribution, which we get by using the binomial coefficient:\n\\[\n{n \\choose x} = \\frac{n!}{x! (n - x)!}\n\\]\nSo the probability (mass) function of any binomial distribution is then this:\n\\[\nf(x) = \\Pr(X = x) = {n \\choose x} p^x (1-p)^{n-x}\n\\]\nThe cumulative distribution function is as follows:\n\\[\nF(x) = \\Pr(X \\leq x) = \\sum_{i = 0}^x {n \\choose x} p^x (1-p)^{n-x}\n\\]\nFor example, with \\(n = 10\\) and \\(p = 0.5\\), this is how they look:\n\n\nCode\ntibble(x = 0:10) |&gt; \n  mutate(\n    \"Probability Mass Function — dbinom(x, size = 10, p = 1/2)\" = \n      dbinom(x, size = 10, p = 1/2),\n    \"Cumulative Distribution Function — pbinom(x, size = 10, p = 1/2)\" = \n      pbinom(x, size = 10, p = 1/2)\n  ) |&gt;\n  mutate(x = factor(x)) |&gt; \n  pivot_longer(!x, names_to = \"distribution\") |&gt; \n  ggplot(aes(x, value)) + \n  geom_col(width = 1/3) +\n  facet_wrap(~distribution, ncol = 1) + \n  labs(y = NULL)\n\n\n\n\n\n\n\n\n\nNote that the Bernoulli distribution is now a special case of the binomial distribution in which \\(n = 1\\).\nThis is what’s going on when you use the dbinom and pbinom functions:\nThe Bernoulli PMF is the same as the binomial PMF with \\(n = 1\\)\n\n\nCode\ndbinom(x = c(-2, -1, 0, 1, 2, 3), size = 1, prob = 0.75)\n\n\n[1] 0.00 0.00 0.25 0.75 0.00 0.00\n\n\nBernoulli CDF\n\n\nCode\npbinom(q = c(-2, -1, 0, 1, 2, 3), size = 1, prob = 0.75)\n\n\n[1] 0.00 0.00 0.25 1.00 1.00 1.00\n\n\nBinomial PMF with \\(n=4\\)\n\n\nCode\ndbinom(x = seq(-1, 5), size = 4, prob = 0.75)\n\n\n[1] 0.00000000 0.00390625 0.04687500 0.21093750 0.42187500 0.31640625 0.00000000\n\n\nBinomial CDF with \\(n=4\\)\n\n\nCode\npbinom(q = seq(-1, 5), size = 4, prob = 0.75)\n\n\n[1] 0.00000000 0.00390625 0.05078125 0.26171875 0.68359375 1.00000000 1.00000000\n\n\nNote that because pbinom is just adding different pieces of dbinom together, we could have obtained the same results simply by adding.\nBinomial CDF with \\(n = 4\\)\n\n\nCode\ncumsum(dbinom(x = seq(-1, 5), size = 4, prob = 0.75))\n\n\n[1] 0.00000000 0.00390625 0.05078125 0.26171875 0.68359375 1.00000000 1.00000000\n\n\nDrawing random samples\nrbinom is used to draw samples from dbinom. This makes doing math very easy. For example, suppose we have 12 coin flips—or a binomial distribution with \\(n = 12\\) and \\(p = 0.5\\).\n\n\nCode\ndraws &lt;- rbinom(n = 1e4, size = 12, prob = 0.5) \n\n\nWhat is the probability that \\(x = 7\\)?\n\n\nCode\nmean(draws == 7)  ## approx\n\n\n[1] 0.1928\n\n\nCode\ndbinom(x = 7, size = 12, prob = 0.5)\n\n\n[1] 0.1933594\n\n\nWhat is the probability that \\(x \\leq 8\\)?\n\n\nCode\nmean(draws &lt;= 8)  ## approx\n\n\n[1] 0.9298\n\n\nCode\npbinom(q = 8, size = 12, prob = 0.5)\n\n\n[1] 0.927002\n\n\nWhat is the probability that \\(x\\) is \\(1\\) or \\(4\\) or \\(9\\)?\n\n\nCode\nmean(draws %in% c(1, 4, 9)) ## approx\n\n\n[1] 0.1785\n\n\nCode\nsum(dbinom(x = c(1, 4, 9), size = 12, prob = 0.5))\n\n\n[1] 0.1774902"
  },
  {
    "objectID": "posts/2022-05-12-distributions-in-r/index.html#continuous-distributions",
    "href": "posts/2022-05-12-distributions-in-r/index.html#continuous-distributions",
    "title": "Distributions in R",
    "section": "continuous distributions",
    "text": "continuous distributions\nThe well-known probability (density) distribution for a normal random variable has two parameters \\(\\mu\\) and \\(\\sigma^2\\).\nIt’s ugly:\n\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\bigg(- \\frac{(x - \\mu)^2}{2 \\sigma^2}\\bigg)\n\\]\nNote that \\(f(x) \\neq \\Pr(X = x)\\).\nBecause \\(x\\) is a real number (that ranges from \\(-\\infty\\) to \\(+\\infty\\)), the probability that \\(x = 1\\) is exactly the same as the probability that \\(x = 0.9999...\\) both are zero.\n\n\nCode\nggplot() + \n  xlim(-5, 5) + \n  geom_function(fun = dnorm) + \n  labs(y = \"density\", x = \"x\")\n\n\n\n\n\n\n\n\n\nHowever, the cumulative distribution function (CDF) does have the same interpretation. If you add all the possible values until you reach \\(x\\) you get \\(\\Pr(X \\leq x)\\). BUT, because there exists an infinite amount of numbers between \\(-\\infty\\) and \\(x\\), you can’t simply add. You have to integrate.\n\\[\nF(x) = \\Pr(X \\leq x) = \\int_{-\\infty}^x f(x) dx\n\\]\n\n\nCode\nggplot() + \n  xlim(-5, 5) + \n  geom_function(fun = pnorm) + \n  labs(y = \"cumulative probability\", x = \"x\")\n\n\n\n\n\n\n\n\n\nAssuming \\(\\mu = 0\\) and \\(\\sigma = 2\\), what is the probability that \\(x\\) is less than or equal to zero?\nThe following two chunks of code give the same answer:\n\n\nCode\npnorm(q = 0, mean = 0, sd = 2)\n\n\n[1] 0.5\n\n\nCode\nintegrate(dnorm, lower = -Inf, upper = 0, mean = 0, sd = 2)\n\n\n0.5 with absolute error &lt; 7.3e-07\n\n\nYou can also get an approximate answer by drawing random samples with rnorm.\n\n\nCode\ndraws &lt;- rnorm(1e5, mean = 0, sd = 2)\nmean(draws &lt;= 0) ## approx\n\n\n[1] 0.49917\n\n\nKnowing that the CDF is an integral, we can understand the PDF as the derivative of the CDF. (The derivative of an integral of a function is just the function itself). In other words, a probability density is the rate of change in cumulative probability at \\(x\\). The PDF is the “slope” of the CDF at \\(x\\). This means that if the cumulative probability is increasing rapidly, the density can easily exceed 1. But if we calculate the area under the density function, it will never exceed 1.\n\nSee the “overthinking” section in McElreath (2020, p. 76) for a similar description of this issue.\n\nFor example, compare the PDF and CDF of the exponential distribution. While the CDF eventually converges to 1, the density easily exceeds 1 at some points.\nCDF:\n\n\nCode\nggplot() + \n  xlim(0, 5) + \n  geom_function(fun = function(x) pexp(x, rate = 3)) + \n  labs(y = \"cumulative probability\", x = \"x\")\n\n\n\n\n\n\n\n\n\nPDF:\n\n\nCode\nggplot() + \n  xlim(0, 5) + \n  geom_function(fun = function(x) dexp(x, rate = 3)) +\n  labs(y = \"density\", x = \"x\")\n\n\n\n\n\n\n\n\n\nMore examples:\n\n\nCode\ndraws &lt;- rnorm(1e4, mean = 2, sd = 5)\n\n\nWhat is the probability that \\(x = 7\\)?\n\n\nCode\nmean(draws == 7)  ## approx\n\n\n[1] 0\n\n\nWhat is the probability that \\(3 &lt; x &lt; 7\\)?\n\n\nCode\nintegrate(dnorm, mean = 2, sd = 5, lower = 3, upper = 7)\n\n\n0.262085 with absolute error &lt; 2.9e-15\n\n\nCode\nmean(3 &lt; draws & draws &lt; 7)\n\n\n[1] 0.2657\n\n\nWhat is the probability that \\(x \\leq 8\\)?\n\n\nCode\nmean(draws &lt;= 8)  ## approx\n\n\n[1] 0.8861\n\n\nCode\npnorm(8, mean = 2, sd = 5)\n\n\n[1] 0.8849303\n\n\nWhat is the probability that \\(x\\) is NOT between \\(-2\\) and \\(2\\)?\n\n\nCode\n1 - integrate(dnorm, mean = 2, sd = 5, lower = -2, upper = 2)$value\n\n\n[1] 0.7118554\n\n\nCode\nmean(draws &lt; -2 | draws &gt; 2)  ## approx\n\n\n[1] 0.7112"
  },
  {
    "objectID": "posts/2022-05-12-distributions-in-r/index.html#quantile-functions",
    "href": "posts/2022-05-12-distributions-in-r/index.html#quantile-functions",
    "title": "Distributions in R",
    "section": "quantile functions",
    "text": "quantile functions\nThe inverse of a CDF is called a quantile function (\\(Q = F^{-1}\\)).\nThis is where we get stuff like the median:\n\\[\n\\underbrace{Q(0.5)}_\\text{median} = x \\iff \\Pr(X \\leq x) = 0.5\n\\]\nMedian example with the exponential distribution:\n\n\nCode\nqexp(p = 0.5, rate = 3)         ## finding the median\n\n\n[1] 0.2310491\n\n\nCode\npexp(q = 0.2310491, rate = 3)   ## verifying the median\n\n\n[1] 0.5000001\n\n\nCode\ndraws &lt;- rexp(1e5, rate = 3)    ## finding the median using random draws\nquantile(draws, 0.5)            ## approx\n\n\n      50% \n0.2317321"
  },
  {
    "objectID": "posts/2021-01-24-optimization-in-r/index.html",
    "href": "posts/2021-01-24-optimization-in-r/index.html",
    "title": "Optimization in R",
    "section": "",
    "text": "Finding the peak of parabola\n\\[y = 15 + 10x - 2x^2\\]\nFirst we write this statement as an R function.\nCode\nparabola &lt;- function(x) 15 + 10*x - 2*x^2\nThen we can visualize it using curve() or ggplot2.\nCode\nlibrary(ggplot2)\ntheme_set(theme_light(base_family = \"Optima\"))\n\ng &lt;- ggplot() + \n  geom_function(fun = parabola) +\n  xlim(0, 5) +\n  labs(x = \"x\", y = \"f(x)\")\n\ng\nThen we call optimize(), which takes the function as its first argument, the interval as its second, and an optional argument indicating whether or not you are searching for the function’s maximum (minimize is the default).\nCode\nout &lt;- optimize(parabola, interval = c(-100, 100), maximum = TRUE)\nout\n\n\n$maximum\n[1] 2.5\n\n$objective\n[1] 27.5\n\n\nCode\ng + geom_vline(xintercept = out$maximum, linetype = \"dashed\") +\n    geom_hline(yintercept = out$objective, linetype = \"dashed\")\nThere are many more ways of using optimization in R. For example, if you want to find the maximum of a function with many parameters you can use optim().\nCode\nopt &lt;- optim(\n  par = 99, ## initial values; use c(...) to do it with many parameters\n  fn = parabola, \n  method = \"BFGS\",\n  # this next line is critical: \n  # it tells R to maximize rather than minimize\n  control = list(fnscale = -1)\n)\n\nopt\n\n\n$par\n[1] 2.5\n\n$value\n[1] 27.5\n\n$counts\nfunction gradient \n       6        3 \n\n$convergence\n[1] 0\n\n$message\nNULL"
  },
  {
    "objectID": "posts/2021-01-24-optimization-in-r/index.html#logistic-regression",
    "href": "posts/2021-01-24-optimization-in-r/index.html#logistic-regression",
    "title": "Optimization in R",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nIn statistics we usually try to find the maximum of likelihood functions in order to fit regression models.\nFor example1, a simple logistic regression can be fit by doing the following:\n\n\nCode\n## The dataset\ndata(\"wells\", package = \"rstanarm\")\nstr(wells)\n\n\n'data.frame':   3020 obs. of  5 variables:\n $ switch : int  1 1 0 1 1 1 1 1 1 1 ...\n $ arsenic: num  2.36 0.71 2.07 1.15 1.1 3.9 2.97 3.24 3.28 2.52 ...\n $ dist   : num  16.8 47.3 21 21.5 40.9 ...\n $ assoc  : int  0 0 0 0 1 1 1 0 1 1 ...\n $ educ   : int  0 0 10 12 14 9 4 10 0 0 ...\n\n\nCode\n## The model\nf &lt;- formula(switch ~ dist + arsenic + dist:arsenic)\n\n## The design matrix\nX &lt;- model.matrix(f, data = wells)\ndim(X)\n\n\n[1] 3020    4\n\n\nCode\n## The outcome variable\ny &lt;- wells$switch\n\n## The log-likelihood function\nlog_likelihood &lt;- function(beta, outcome, dmat) {\n  \n  linpred &lt;- dmat %*% beta ## the linear predictor\n  p &lt;- plogis(linpred)     ## the link function\n  \n  sum(dbinom(outcome, size = 1, prob = p, log = TRUE))  ## the log-likelihood\n}\n\n## The maximum likelihood estimate (MLE)\nopt &lt;- optim(\n  par = rep(0, ncol(X)), ## initial values are all 0's\n  fn = log_likelihood, method = \"BFGS\",\n  outcome = y, dmat = X,\n  # this next line is critical: \n  # it tells R to maximize rather than minimize\n  control = list(fnscale = -1)\n)\n\nnames(opt$par) &lt;- colnames(X)\nopt$par\n\n\n (Intercept)         dist      arsenic dist:arsenic \n-0.147139626 -0.005811349  0.555574583 -0.001768792 \n\n\nWe can compare this to the outcome given by R’s glm() function:\n\n\nCode\nfit &lt;- glm(f, data = wells, family = binomial(link = \"logit\"))\ncoefficients(fit)\n\n\n (Intercept)         dist      arsenic dist:arsenic \n-0.147868069 -0.005772178  0.555976744 -0.001789060"
  },
  {
    "objectID": "posts/2021-01-24-optimization-in-r/index.html#stan",
    "href": "posts/2021-01-24-optimization-in-r/index.html#stan",
    "title": "Optimization in R",
    "section": "Stan",
    "text": "Stan\nUsers of Stan should know that it can be used for optimization as well.\n\n\nCode\nlibrary(cmdstanr)\nmcmc_optim &lt;- cmdstan_model(\"simple_parabola.stan\")\nmcmc_optim$print()\n\n\nparameters {\n  real&lt;lower=0&gt; x; // easily does constrained optimization\n}\n\nmodel {\n  target += 15 + 10*x - 2*x^2;\n}\n\n\nCode\nfit &lt;- mcmc_optim$optimize()\n\n\nInitial log joint probability = 17.7521 \n    Iter      log prob        ||dx||      ||grad||       alpha      alpha0  # evals  Notes  \n       4          27.5    0.00019008   2.86238e-06           1           1       12    \nOptimization terminated normally:  \n  Convergence detected: relative gradient magnitude is below tolerance \nFinished in  0.3 seconds.\n\n\nCode\nfit\n\n\n variable estimate\n     lp__    27.50\n     x        2.50"
  },
  {
    "objectID": "posts/2021-01-24-optimization-in-r/index.html#footnotes",
    "href": "posts/2021-01-24-optimization-in-r/index.html#footnotes",
    "title": "Optimization in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe data: «A survey of 3020 residents in a small area of Bangladesh suffering from arsenic contamination of groundwater. Respondents with elevated arsenic levels in their wells had been encouraged to switch their water source to a safe public or private well in the nearby area and the survey was conducted several years later to learn which of the affected residents had switched wells»↩︎"
  },
  {
    "objectID": "posts/2022-01-01-regression-to-the-mean/index.html",
    "href": "posts/2022-01-01-regression-to-the-mean/index.html",
    "title": "Regression to the Mean",
    "section": "",
    "text": "Regression to the mean is one of the greatest parables in statistics. The term originated in 1886, when Francis Galton was studying hereditary patterns in human populations. While he was studying the association between children’s and parent’s heights, he noticed that tall (short) parents tend to have children shorter (taller) than themselves; and that they tended towards a population average.\n\npar-a-ble: A simple story used to illustrate a moral or spiritual lesson, as told by Jesus in the Gospels.\n\n\n\nCode\nHistData::Galton |&gt; \n  ggplot(aes(x = parent, y = child)) + \n  geom_jitter(alpha = 0.5) +\n  ## line of equality\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  ## regression line\n  geom_smooth(method = lm, se = FALSE, color = \"skyblue\") \n\n\n\n\n\n\n\n\n\nThis is the data originally analyzed by Galton himself. The dashed line represents equality, meaning that parents and children have the same height. But Galton noted that children’s heights were actually closer to the blue prediction line.\n\nSee Stigler (2016).\n\nThe key insight is that, whenever randomness is involved, the more extreme outcomes will tend to be followed by more moderate outcomes. Why? Because part of the reason these outcomes are so extreme in the first place is due to randomness. Galton also termed this phenomena “regression to mediocrity”1.\nNowadays, we would look at this same problem using linear regression, which allows us to put the predicted heights of children into a simple formula.\n\n\nCode\nd &lt;- HistData::Galton |&gt; \n  mutate(parent_centered = parent - mean(parent))\n\nOLS &lt;- lm(child ~ parent_centered, data = d) ## scaling\n\ncoefficients(OLS)\n\n\n    (Intercept) parent_centered \n     68.0884698       0.6462906 \n\n\nNote that the heights of parents have been centered in order to produce an intercept that corresponds to the population average (or “mediocrity”).\n\\[\n\\widehat h_\\text{child} =  \\underbrace{68.09}_\\text{mediocrity} + 0.65 \\times \\widetilde h_\\text{parent}\n\\]\nAt first encounter, some people will interpret this as meaning that regression to the mean implies that heights will become “more average” over time. Thus, a parent who is 5 inches above average is predicted to have children 3.2 inches taller than average, who in turn are predicted to have children 2.1 inches taller than average, and so on. This very common misunderstanding arises from neglecting the error term when thinking about future observations.\n\\[\nh_\\text{child} = \\widehat h_\\text{child} + \\text{error}\n\\]\nIn Gelman et al’s words:\n\nThe point predictions regress toward the mean—that’s the coefficient less than 1—and this reduces variation. At the same time, though, the error in the model—the imperfection of the prediction—adds variation, just enough to keep the total variation in height roughly constant from one generation to the next.\nRegression to the mean thus will always arise in some form whenever predictions are imperfect in a stable environment. The imperfection of the prediction induces variation, and regression in the point prediction is required in order to keep the total variation constant.\nGelman et al. (2020, p. 88)\n\nThe following graphs show both ways of interpreting regression to the mean, based on how the data could look like across 12 generations:\n\n\nCode\nOLS &lt;- lm(child ~ parent, data = HistData::Galton)\nintercept &lt;- coefficients(OLS)[[1]]\nslope &lt;- coefficients(OLS)[[2]]\nsigma &lt;- sd(residuals(OLS))\n\ng0 &lt;- HistData::Galton$parent # first ancestors\n\n## Simulation\n\nreg_naive &lt;- function(x, ...) {\n  x * slope + intercept\n}\n\nreg_correct &lt;- function(x, ...) {\n  rnorm(n = length(x), mean = x * slope + intercept, sd = sigma)\n}\n\nn &lt;- 12 # number of generations\n\ns_naive &lt;- accumulate(1:n, reg_naive, .init = g0) \ns_correct &lt;- accumulate(1:n, reg_correct, .init = g0)\nnames(s_correct) &lt;- names(s_naive) &lt;- 0:n\n\ndf_naive &lt;- as_tibble(s_naive) |&gt; \n  mutate(ancestor = row_number()) |&gt; \n  pivot_longer(cols = !ancestor, names_to = \"generation\", values_to = \"height\") |&gt; \n  mutate(simulation = \"naive interpretation\", generation = as.integer(generation))\n\ndf_correct &lt;- as_tibble(s_correct) |&gt; \n  mutate(ancestor = row_number()) |&gt; \n  pivot_longer(cols = !ancestor, names_to = \"generation\", values_to = \"height\") |&gt; \n  mutate(simulation = \"correct interpretation\", generation = as.integer(generation))\n\nbind_rows(df_correct, df_naive) |&gt; \n  ggplot(aes(x = generation, y = height, color = ancestor, group = ancestor)) +\n  geom_line(show.legend = FALSE, alpha = 1/10) +\n  facet_wrap(~ simulation, ncol = 1, scales = \"free_y\") + \n  labs(y = \"height (inches)\") + \n  scale_color_viridis_c() +\n  scale_x_continuous(breaks = 0:12) +\n  theme(strip.text.x = element_text(size = 14)) \n\n\n\n\n\n\n\n\n\n\nThis very simple simulation assumes that the population average remains constant over time, which is obviously false when you consider changes in public health, such as nutrition.\nAs a general rule, nothing in the social and biological sciences remains constant over very long periods of time."
  },
  {
    "objectID": "posts/2022-01-01-regression-to-the-mean/index.html#origin-story",
    "href": "posts/2022-01-01-regression-to-the-mean/index.html#origin-story",
    "title": "Regression to the Mean",
    "section": "",
    "text": "Regression to the mean is one of the greatest parables in statistics. The term originated in 1886, when Francis Galton was studying hereditary patterns in human populations. While he was studying the association between children’s and parent’s heights, he noticed that tall (short) parents tend to have children shorter (taller) than themselves; and that they tended towards a population average.\n\npar-a-ble: A simple story used to illustrate a moral or spiritual lesson, as told by Jesus in the Gospels.\n\n\n\nCode\nHistData::Galton |&gt; \n  ggplot(aes(x = parent, y = child)) + \n  geom_jitter(alpha = 0.5) +\n  ## line of equality\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  ## regression line\n  geom_smooth(method = lm, se = FALSE, color = \"skyblue\") \n\n\n\n\n\n\n\n\n\nThis is the data originally analyzed by Galton himself. The dashed line represents equality, meaning that parents and children have the same height. But Galton noted that children’s heights were actually closer to the blue prediction line.\n\nSee Stigler (2016).\n\nThe key insight is that, whenever randomness is involved, the more extreme outcomes will tend to be followed by more moderate outcomes. Why? Because part of the reason these outcomes are so extreme in the first place is due to randomness. Galton also termed this phenomena “regression to mediocrity”1.\nNowadays, we would look at this same problem using linear regression, which allows us to put the predicted heights of children into a simple formula.\n\n\nCode\nd &lt;- HistData::Galton |&gt; \n  mutate(parent_centered = parent - mean(parent))\n\nOLS &lt;- lm(child ~ parent_centered, data = d) ## scaling\n\ncoefficients(OLS)\n\n\n    (Intercept) parent_centered \n     68.0884698       0.6462906 \n\n\nNote that the heights of parents have been centered in order to produce an intercept that corresponds to the population average (or “mediocrity”).\n\\[\n\\widehat h_\\text{child} =  \\underbrace{68.09}_\\text{mediocrity} + 0.65 \\times \\widetilde h_\\text{parent}\n\\]\nAt first encounter, some people will interpret this as meaning that regression to the mean implies that heights will become “more average” over time. Thus, a parent who is 5 inches above average is predicted to have children 3.2 inches taller than average, who in turn are predicted to have children 2.1 inches taller than average, and so on. This very common misunderstanding arises from neglecting the error term when thinking about future observations.\n\\[\nh_\\text{child} = \\widehat h_\\text{child} + \\text{error}\n\\]\nIn Gelman et al’s words:\n\nThe point predictions regress toward the mean—that’s the coefficient less than 1—and this reduces variation. At the same time, though, the error in the model—the imperfection of the prediction—adds variation, just enough to keep the total variation in height roughly constant from one generation to the next.\nRegression to the mean thus will always arise in some form whenever predictions are imperfect in a stable environment. The imperfection of the prediction induces variation, and regression in the point prediction is required in order to keep the total variation constant.\nGelman et al. (2020, p. 88)\n\nThe following graphs show both ways of interpreting regression to the mean, based on how the data could look like across 12 generations:\n\n\nCode\nOLS &lt;- lm(child ~ parent, data = HistData::Galton)\nintercept &lt;- coefficients(OLS)[[1]]\nslope &lt;- coefficients(OLS)[[2]]\nsigma &lt;- sd(residuals(OLS))\n\ng0 &lt;- HistData::Galton$parent # first ancestors\n\n## Simulation\n\nreg_naive &lt;- function(x, ...) {\n  x * slope + intercept\n}\n\nreg_correct &lt;- function(x, ...) {\n  rnorm(n = length(x), mean = x * slope + intercept, sd = sigma)\n}\n\nn &lt;- 12 # number of generations\n\ns_naive &lt;- accumulate(1:n, reg_naive, .init = g0) \ns_correct &lt;- accumulate(1:n, reg_correct, .init = g0)\nnames(s_correct) &lt;- names(s_naive) &lt;- 0:n\n\ndf_naive &lt;- as_tibble(s_naive) |&gt; \n  mutate(ancestor = row_number()) |&gt; \n  pivot_longer(cols = !ancestor, names_to = \"generation\", values_to = \"height\") |&gt; \n  mutate(simulation = \"naive interpretation\", generation = as.integer(generation))\n\ndf_correct &lt;- as_tibble(s_correct) |&gt; \n  mutate(ancestor = row_number()) |&gt; \n  pivot_longer(cols = !ancestor, names_to = \"generation\", values_to = \"height\") |&gt; \n  mutate(simulation = \"correct interpretation\", generation = as.integer(generation))\n\nbind_rows(df_correct, df_naive) |&gt; \n  ggplot(aes(x = generation, y = height, color = ancestor, group = ancestor)) +\n  geom_line(show.legend = FALSE, alpha = 1/10) +\n  facet_wrap(~ simulation, ncol = 1, scales = \"free_y\") + \n  labs(y = \"height (inches)\") + \n  scale_color_viridis_c() +\n  scale_x_continuous(breaks = 0:12) +\n  theme(strip.text.x = element_text(size = 14)) \n\n\n\n\n\n\n\n\n\n\nThis very simple simulation assumes that the population average remains constant over time, which is obviously false when you consider changes in public health, such as nutrition.\nAs a general rule, nothing in the social and biological sciences remains constant over very long periods of time."
  },
  {
    "objectID": "posts/2022-01-01-regression-to-the-mean/index.html#the-lesson",
    "href": "posts/2022-01-01-regression-to-the-mean/index.html#the-lesson",
    "title": "Regression to the Mean",
    "section": "The lesson",
    "text": "The lesson\nRegression to the mean will trick people into seeing causality where there is none.\nAs mentioned earlier, this phenomena shows up all the time wherever randomness (or “luck”) is expected to play a role. Thus, we see it in everything from sports to public policy.2 This is a problem because human beings are inclined to find amazing patterns when they look at randomness. Obviously, this is mostly harmless when, for example, the source of randomness is clouds in the sky and the patterns we find take the form of “a cow” or “a face.”3 But in other settings, this same phenomena can become worrisome.\nAn exercise in simulation4\nA very famous real-world example is contained in an article titled “On the psychology of prediction” Tversky & Kahneman, 1973):\n\nThe instructors in a flight school adopted a policy of consistent positive reinforcement recommended by psychologists. They verbally reinforced each successful execution of a flight maneuver. After some experience with this training approach, the instructors claimed that contrary to psychological doctrine, high praise for good execution of complex maneuvers typically results in a decrement of performance on the next try. What should the psychologist say in response?\n\nTo answer this question we simulate data from 500 pilots, each of whom performs two maneuvers, and with each maneuver scored continuously on a 0-10 scale. Each pilot has a “true ability” that is unchanged during the two tasks, and the score for each test is equal to this true ability plus an independent error. Pilots get praised when they score higher than 7 during the first maneuver, and receive negative reinforcement when they score lower than 3.\nThe connection between this example and Galton’s study is not obvious at first glance. Here, each pilot’s score exhibits regression to her “true ability”, in the sense that we expect “luck” to play an important role. As before, we don’t really expect these “true abilities” to remain constant over time. In fact that would defeat the entire purpose of flight school. But we do expect these underlying abilities to change very slowly.\nNote that we make sure, by design, that reinforcement has no effect on performance on the second task.\n\n\nCode\nN &lt;- 500\ntrue_abilities &lt;- rnorm(N, 0, 1)\n# mapping the true abilities + random noise to a [0,10] scale\ntask1 &lt;- plogis(true_abilities + rnorm(N, 0, 0.5)) * 10\ntask2 &lt;- plogis(true_abilities + rnorm(N, 0, 0.5)) * 10\n\ndf &lt;- tibble(true_abilities, task1, task2) |&gt; \n  mutate(reinforcement = case_when(\n    task1 &lt; 3 ~ \"negative\",\n    task1 &gt; 7 ~ \"positive\",\n    TRUE ~ \"neutral\")\n  )\n\ndf |&gt; \n  ggplot(aes(x = task1, y = task2, color = reinforcement)) +\n  geom_point() +\n  scale_color_manual(values = c(\"pink\", \"grey\", \"skyblue\")) +\n  scale_x_continuous(breaks = seq(2, 10, 2)) +\n  scale_y_continuous(breaks = seq(2, 10, 2))\n\n\n\n\n\n\n\n\n\nWe can now compute the average change in scores for each group of pilots, which the instructors interpret to be the causal effect of reinforcement.\n\n\nCode\ndf |&gt; \n  mutate(change = task2 - task1) |&gt; \n  group_by(reinforcement) |&gt; \n  summarise(\n    effect = mean(change),\n    se = sd(change) / sqrt(n())\n    ) |&gt; \n  ggplot(aes(x = reinforcement, y = effect)) +\n  geom_pointrange(aes(\n    ymin = effect + qnorm(p = 0.025)*se,\n    ymax = effect + qnorm(p = 0.975)*se\n    )) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nNotice that, on average, the pilots who were praised did worse on the second task, whereas the pilots who received negative reinforcement did better. But we know that such effect doesn’t exist. And we know this because we created these data so that task 1 and task 2 were unrelated. The “causal pattern” we have just observed is a consequence of the noise in the data. Pilots who scored very well on task 1 are likely to have a higher skill and also to have been somewhat lucky. Thus, it makes sense that they perform slightly worse on task 2.\nThis is how Tversky and Kahneman (1982) explain it:\n\nRegression is inevitable in flight maneuvers because performance is not perfectly reliable and progress between successive maneuvers is slow. Hence, pilots who did exceptionally well on one trial are likely to deteriorate on the next, regardless of the instructors’ reaction to the initial success. The experienced flight instructors actually discovered the regression but attributed it to the detrimental effect of positive reinforcement. This true story illustrates a saddening aspect of the human condition. We normally reinforce others when their behavior is good and punish them when their behavior is bad. By regression alone, therefore, they are most likely to improve after being punished and most likely to deteriorate after being rewarded. Consequently, we are exposed to a lifetime schedule in which we are most often rewarded for punishing others, and punished for rewarding.\nQuoted in Gelman et al. (2020, p. 90)"
  },
  {
    "objectID": "posts/2022-01-01-regression-to-the-mean/index.html#footnotes",
    "href": "posts/2022-01-01-regression-to-the-mean/index.html#footnotes",
    "title": "Regression to the Mean",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHistorically, there have been two kinds moral judgments regarding averages and normal distributions (Hacking 1990): (1) the Quetelet-Durkheim conception of the normal as the right and the good; and (2) Galton’s notion of the normal as the mediocre, and in need of improvement.↩︎\nThis also seems to be the reason why so many bands appear to suffer from “second album syndrome” or “sophomore slump”.↩︎\nSee: pareidolia.↩︎\nThis my solution to exercise 6.8 in Regression and Other Stories.↩︎"
  }
]